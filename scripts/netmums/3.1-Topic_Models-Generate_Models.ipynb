{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Topic Models\n",
    "Generates the topic models of forum posts with LDA (Latent Dirichlet Allocation)\n",
    "\n",
    "## Data Sources\n",
    "- corpus (created with 3.0-Topic_Models-Lemmatize_Text.ipynb)\n",
    "- dictionary (created with 3.0-Topic_Models-Lemmatize_Text.ipynb)\n",
    "- lemmatized_text (created with 3.0-Topic_Models-Lemmatize_Text.ipynb)\n",
    "\n",
    "## TODO\n",
    "- Tutorial\n",
    " - https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21\n",
    " - https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "from gensim import corpora, models\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models import CoherenceModel, LdaModel, LdaMulticore\n",
    "from gensim.models.callbacks import PerplexityMetric, ConvergenceMetric, CoherenceMetric\n",
    "# Managing data\n",
    "import pandas as pd\n",
    "import re\n",
    "# DB connection\n",
    "from scraping import create_connection\n",
    "# Files & I/O\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from io import FileIO\n",
    "# For logging\n",
    "import logging\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For formatting LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list(fn, results):\n",
    "    with open(fn, 'a') as f:\n",
    "        writer = csv.writer(f) \n",
    "        writer.writerow(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_topics(topics):\n",
    "    return [t[1] for t in topics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(forum=\"all\", group=\"all\", id_type=\"family_id\"):\n",
    "    lemmatized_text = pickle.load(open(path_lemma_pkl.format(forum, group, id_type), 'rb'))\n",
    "    corpus = pickle.load(open(path_corpus_pkl.format(forum, group, id_type), 'rb'))\n",
    "    dictionary = corpora.Dictionary.load(path_dictionary_gensim.format(forum, group, id_type))\n",
    "    return lemmatized_text, corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_doc_convergence(log, i):\n",
    "    # Regex to bookend log for iteration - choose last occurrence\n",
    "#     end_slice = re.compile(fr\"End of model: {i} iterations\")\n",
    "#     end_matches = [end_slice.findall(l) for l in open(log)]\n",
    "#     iteration_end = [i for i, x in enumerate(end_matches) if x]\n",
    "#     iteration_end = iteration_end[-1]\n",
    "#     start_slice = re.compile(fr\"Start of model: {i} iterations\")\n",
    "#     start_matches = [start_slice.findall(l) for l in open(log)]\n",
    "#     start_options = [i for i, x in enumerate(start_matches) if x]\n",
    "#     start_options = [item for item in start_options if item < iteration_end]\n",
    "#     iteration_start = max(start_options)\n",
    "#     iteration_bookends = [iteration_start, iteration_end]\n",
    "    # Regex to find documents converged figures\n",
    "    num = re.compile(\":(\\d+)\\/\\d\")\n",
    "    matches_num = [num.findall(l) for l in open(log)]\n",
    "#     matches_num = matches_num[iteration_bookends[0]:iteration_bookends[1]]\n",
    "    matches_num = [m for m in matches_num if len(m) > 0]\n",
    "    # Unlist internal lists and turn into numbers\n",
    "    matches_num = [m for sublist in matches_num for m in sublist]\n",
    "    matches_num = [float(m) for m in matches_num]\n",
    "    # Regex to find documents converged figures\n",
    "    den = re.compile(\":\\d+\\/(\\d+)\")\n",
    "    matches_den = [den.findall(l) for l in open(log)]\n",
    "#     matches_den = matches_den[iteration_bookends[0]:iteration_bookends[1]]\n",
    "    matches_den = [m for m in matches_den if len(m) > 0]\n",
    "    # Unlist internal lists and turn into numbers\n",
    "    matches_den = [m for sublist in matches_den for m in sublist]\n",
    "    matches_den = [float(m) for m in matches_den]\n",
    "    return(matches_num, matches_den)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path.cwd()\n",
    "path_parent = p.parents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database\n",
    "path_db = str(path_parent / \"database\" / \"youbemom-merged.db\")\n",
    "path_clean_data = path_parent / \"clean_data\" / \"netmums\"\n",
    "# data to load\n",
    "path_lemma_pkl = str(path_clean_data / \"lemmatized_text_{0}_{1}_{2}.pkl\")\n",
    "path_corpus_pkl = str(path_clean_data / \"corpus_{0}_{1}_{2}.pkl\")\n",
    "path_dictionary_gensim = str(path_clean_data / \"dictionary_{0}_{1}_{2}.gensim\")\n",
    "# model saving\n",
    "path_tune_models = str(path_clean_data / \"lda_tune_{0}_{1}_{2}_{3}_{4}.gensim\")\n",
    "path_ntopic_models = str(path_clean_data / \"lda_ntopics_{0}_{1}_{2}_{3}.gensim\")\n",
    "# path_coherence = str(path_parent / \"clean_data\" / \"coherence_{}.csv\")\n",
    "path_log = str(path_clean_data / \"logging_{0}_{1}_{2}_{3}.log\")\n",
    "path_log_iterations = str(path_clean_data / \"logging_{0}_{1}_{2}_{3}.log\")\n",
    "# dominant topic\n",
    "path_dom_topic = str(path_clean_data / \"dominant_topic_{0}_{1}_{2}_{3}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model for convergence\n",
    "Train an LDA model on all subforums and all posts grouped on family_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum = ['being-a-mum'] # ['special-needs','tween-teen','preschool','elementary','new-york-city','toddler']\n",
    "group = 'all'\n",
    "id_type = 'family_id'\n",
    "n_words = 10\n",
    "n_passes = 30\n",
    "n_iterations = [50, 100] # add more to list to test\n",
    "eval_every = 20\n",
    "n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 10 topics to evaluate number of passes and iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in forum:\n",
    "    lemmatized_text, corpus, dictionary = load_data(f, group, id_type)\n",
    "    for handler in logging.root.handlers:\n",
    "        logging.root.removeHandler(handler)\n",
    "    logging.basicConfig(filename=path_log.format(f, group, id_type, n),\n",
    "                        format=\"%(asctime)s:%(levelname)s:%(message)s\",\n",
    "                        level=logging.NOTSET)\n",
    "    perplexity_logger = PerplexityMetric(corpus=corpus, logger='shell')\n",
    "    convergence_logger = ConvergenceMetric(logger='shell')\n",
    "    coherence_cv_logger = CoherenceMetric(corpus=corpus, logger='shell', coherence = 'c_v', texts = lemmatized_text)\n",
    "    for iterations in n_iterations:\n",
    "        logging.debug(f'Start of model: {iterations} iterations')\n",
    "        ldamodel = LdaModel(\n",
    "            corpus=corpus,\n",
    "            num_topics=n,\n",
    "            id2word=dictionary,\n",
    "            passes=n_passes,\n",
    "            alpha=\"auto\",\n",
    "            eta=\"auto\",\n",
    "            random_state=1,\n",
    "            iterations=iterations,\n",
    "            eval_every=eval_every,\n",
    "            callbacks=[perplexity_logger, convergence_logger, coherence_cv_logger]\n",
    "        )\n",
    "        logging.debug(f'End of model: {iterations} iterations')\n",
    "        ldamodel.save(path_tune_models.format(f, group, id_type, str(n), str(iterations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart convergence of 10 topics\n",
    "see: https://www.meganstodel.com/posts/callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"being-a-mum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = LdaModel.load(path_tune_models.format(f, group, id_type, str(n), str(50)))\n",
    "df = pd.DataFrame.from_dict(ldamodel.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.index, df[\"Convergence\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(df.index, df[\"Coherence\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.index, df[\"Perplexity\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_num, con_den = find_doc_convergence(path_log.format(f, group, id_type, n), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(con_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_points = list(range(37)) * 198\n",
    "epochs = [i for i in range(198) for _ in range(37)]\n",
    "convergence = pd.DataFrame(list(zip(epochs, eval_points, con_num, con_den)),\n",
    "                               columns = [\"epoch\",\"eval_point\",\"converged\",\"total\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convergence['epoch_point'] = convergence['epoch'] + convergence['eval_point'] / 13\n",
    "convergence['per_converged'] = convergence['converged'] / convergence['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(convergence['epoch_point'], convergence['per_converged'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For special needs: after testing 10, 100, and 200 iterations, we need at least 200 for the docs made from threads (grouped on family_id) to converge. 100 passes seems to let the convergence, perplexity, and coherence converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through different topic counts to compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'special-needs'\n",
    "group = 'all'\n",
    "id_type = 'family_id'\n",
    "n_words = 10\n",
    "n_passes = 30\n",
    "n_iterations = 50\n",
    "n_topics = [6, 7, 8, 9]\n",
    "# n_topics = [5, 10, 15, 20, 25, 30, 40, 50]\n",
    "# n_topics = [40, 50]\n",
    "eval_every = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_text, corpus, dictionary = load_data(f, group, id_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of topics:  6\n",
      "number of topics:  7\n",
      "number of topics:  8\n",
      "number of topics:  9\n"
     ]
    }
   ],
   "source": [
    "for n in n_topics:\n",
    "    print(\"number of topics: \", n)\n",
    "    ldamodel = LdaModel(\n",
    "        corpus=corpus,\n",
    "        num_topics=n,\n",
    "        id2word=dictionary,\n",
    "        passes=n_passes,\n",
    "        alpha=\"auto\",\n",
    "        eta=\"auto\",\n",
    "        random_state=1,\n",
    "        iterations=n_iterations,\n",
    "        eval_every=eval_every\n",
    "    )\n",
    "    ldamodel.save(path_ntopic_models.format(f, group, id_type, str(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_topics = str(path_clean_data / \"lda_topics_{0}_{1}_{2}.csv\")\n",
    "write_list(path_topics.format(f, group, id_type), [\"n_topics\",\"topic_n\",\"topics\"])\n",
    "for n in n_topics:\n",
    "    ldamodel = LdaModel.load(path_ntopic_models.format(f, group, id_type, str(n)))\n",
    "    topics = ldamodel.print_topics(num_topics=n, num_words=n_words)\n",
    "    for topic in topics:\n",
    "        write_list(path_topics.format(f, group, id_type), [n, topic[0], topic[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_dict = {}\n",
    "for i in n_topics:\n",
    "    mod_dict[str(i)] = LdaModel.load(path_ntopic_models.format(f, group, id_type, str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5: 0.4981136066898356\n",
      "10: 0.44341963839966325\n",
      "15: 0.42486733865512466\n",
      "20: 0.3714477773121006\n",
      "25: 0.3769020507005674\n",
      "30: 0.39346690561440556\n",
      "40: 0.37022300365213134\n",
      "50: 0.34779406672652774\n"
     ]
    }
   ],
   "source": [
    "for i, mod in mod_dict.items():\n",
    "    coherence_model_lda = CoherenceModel(model=mod, texts=lemmatized_text, dictionary=dictionary, coherence='c_v')\n",
    "    print(\"{}:\".format(i), coherence_model_lda.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1515641399381230276164322958571\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1515641399381230276164322958571_data = {\"mdsDat\": {\"x\": [0.3249568777204675, -0.183015059794194, 0.26007155505669804, 0.14510118882354284, -0.2316239597308564, -0.2248906887867634, -0.20261973243572126, 0.1710030860435135, 0.03458152198245589, -0.09356478887914348], \"y\": [0.0953629495057648, 0.04280364027238481, -0.10900428049171017, -0.21989302615605918, 0.03407531832355899, 0.035295526031679535, 0.03685600347481004, 0.3464065230919104, -0.182158075886742, -0.07974457816559737], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [65.32602868693147, 0.905333824244562, 13.019542777196119, 6.76326483017464, 0.27249273808498853, 0.4032262233098904, 0.6853906906526036, 7.6302720626618346, 3.42559752308313, 1.5688506436607537]}, \"tinfo\": {\"Term\": [\"child\", \"help\", \"im\", \"need\", \"he\", \"hi\", \"hope\", \"week\", \"support\", \"thanks\", \"may\", \"get\", \"x\", \"try\", \"like\", \"ive\", \"school\", \"use\", \"parent\", \"statement\", \"care\", \"would\", \"able\", \"also\", \"doesnt\", \"son\", \"know\", \"sorry\", \"hear\", \"anyone\", \"get\", \"school\", \"son\", \"say\", \"one\", \"think\", \"dont\", \"take\", \"want\", \"feel\", \"problem\", \"really\", \"daughter\", \"us\", \"way\", \"seem\", \"ds\", \"getting\", \"come\", \"hard\", \"2\", \"autism\", \"didnt\", \"3\", \"home\", \"anything\", \"behaviour\", \"teacher\", \"kid\", \"understand\", \"know\", \"time\", \"well\", \"start\", \"go\", \"still\", \"things\", \"see\", \"much\", \"tell\", \"make\", \"work\", \"like\", \"would\", \"going\", \"also\", \"etc\", \"try\", \"could\", \"help\", \"child\", \"he\", \"sn\", \"doc\", \"med\", \"dry\", \"na\", \"muscle\", \"tj\", \"spelling\", \"dyspraxic\", \"ca\", \"app\", \"ace\", \"itll\", \"jackson\", \"wen\", \"bloody\", \"iam\", \"60\", \"wipe\", \"laptop\", \"leaflet\", \"jane\", \"wont_eat\", \"disability_team\", \"keep_pushing\", \"pull_up\", \"frame\", \"10_min\", \"alfie\", \"hell\", \"support\", \"may\", \"hug\", \"emma\", \"area\", \"special_need\", \"local\", \"offer\", \"helpful\", \"condition\", \"website\", \"professional\", \"provide\", \"education\", \"link\", \"team\", \"perhaps\", \"support_group\", \"sen\", \"site\", \"service\", \"centre\", \"useful\", \"board\", \"sorry_hear\", \"recommend\", \"access\", \"funding\", \"best_wish\", \"services\", \"parent\", \"meet\", \"able\", \"important\", \"information\", \"contact\", \"group\", \"family\", \"include\", \"need\", \"child\", \"disability\", \"help\", \"experience\", \"find\", \"sound_like\", \"please\", \"also\", \"would\", \"advice\", \"care\", \"many\", \"look\", \"hope\", \"could\", \"diagnosis\", \"might\", \"im\", \"thanks\", \"x\", \"ive\", \"hear\", \"xx\", \"thank\", \"emojismile\", \"soon\", \"hopefully\", \"reply\", \"ill\", \"hun\", \"emojifrowning\", \"lady\", \"dx\", \"tomorrow\", \"xxx\", \"thanks_reply\", \"sarah\", \"pm\", \"emojilaughing\", \"next_week\", \"glad\", \"monday\", \"josh\", \"message\", \"thankyou\", \"gonna\", \"guy\", \"today\", \"lol\", \"post\", \"hope\", \"hi\", \"sorry\", \"waiting\", \"im_sure\", \"nice\", \"wonder\", \"anyone\", \"lovely\", \"thats\", \"hiya\", \"advice\", \"everyone\", \"please\", \"great\", \"8_week\", \"register\", \"twins\", \"lift\", \"good_news\", \"kelly\", \"rebecca\", \"much_longer\", \"download\", \"text\", \"boards\", \"asperger_syndrome\", \"air\", \"county\", \"aba\", \"roughly\", \"credit\", \"answer_soon\", \"needle\", \"cerebrum\", \"playschool\", \"gross_motor\", \"project\", \"foundation\", \"dependent\", \"avenue\", \"conference\", \"wandering\", \"america\", \"comply\", \"n\", \"ds1\", \"dh\", \"ds2\", \"save\", \"lisa\", \"document\", \"infection\", \"country\", \"keep_telling\", \"sara\", \"sunday\", \"po\", \"advisor\", \"hi_kelly\", \"patch\", \"james\", \"france\", \"prescription\", \"shirt\", \"oct\", \"aww\", \"gettin\", \"grommet\", \"massage\", \"sum\", \"cf\", \"appeal_decision\", \"qualification\", \"0\", \"u\", \"contact_family\", \"ur\", \"lucy\", \"xxxx\", \"r\", \"cheap\", \"hi_michelle\", \"im_afraid\", \"tube\", \"thomas\", \"ruth\", \"alternative\", \"sore\", \"ha_ha\", \"fragile_x\", \"threaten\", \"tie\", \"visiting\", \"bt\", \"manager\", \"instinct\", \"symbol\", \"liz_xx\", \"member_staff\", \"whats_best\", \"ward\", \"easter\", \"tesco\", \"cream\", \"baby\", \"sleep\", \"night\", \"walk\", \"bed\", \"house\", \"room\", \"sit\", \"food\", \"buy\", \"calm\", \"toy\", \"walking\", \"cry\", \"eat\", \"pain\", \"hurt\", \"watch\", \"playing\", \"toilet\", \"noise\", \"foot\", \"wear\", \"throw\", \"nappy\", \"hands\", \"game\", \"tire\", \"jack\", \"laugh\", \"play\", \"brother\", \"car\", \"sister\", \"hate\", \"constantly\", \"use\", \"stop\", \"love\", \"doesnt\", \"try\", \"run\", \"like\", \"cant\", \"put\", \"day\", \"statement\", \"form\", \"report\", \"phone\", \"decision\", \"letter\", \"sent\", \"send\", \"ring\", \"appeal\", \"date\", \"payment\", \"fill\", \"application\", \"iep\", \"la\", \"evidence\", \"letters\", \"tribunal\", \"july\", \"june\", \"claim_dla\", \"april\", \"contact_local\", \"department\", \"wednesday\", \"phone_call\", \"recieved\", \"statutory_assessment\", \"paperwork\", \"apply\", \"copy\", \"receive\", \"request\", \"claim\", \"award\", \"state\", \"week\", \"write\", \"review\", \"dla\", \"call\", \"process\", \"ask\", \"pay\", \"money\", \"physio\", \"entitle\", \"mobility\", \"parent_partnership\", \"allowance\", \"carers_allowance\", \"backdate\", \"carer\", \"boot\", \"high_rate\", \"tax_credits\", \"middle_rate\", \"oops\", \"ben\", \"rate\", \"alex\", \"apply_carers\", \"blue\", \"transport\", \"fill_form\", \"equipment\", \"claim_carers\", \"income_support\", \"hth\", \"referal\", \"ss\", \"back_date\", \"jeanc\", \"dla\", \"carers\", \"benefit\", \"claim\", \"care\", \"award\", \"apply\"], \"Freq\": [119788.0, 99826.0, 47621.0, 78953.0, 26352.0, 51951.0, 39478.0, 30712.0, 41003.0, 27963.0, 36524.0, 174131.0, 25698.0, 52532.0, 73293.0, 20615.0, 127083.0, 30161.0, 26506.0, 15145.0, 15684.0, 92027.0, 23926.0, 72018.0, 28888.0, 108647.0, 110429.0, 18084.0, 16432.0, 18415.0, 174130.7002847877, 127082.99737906108, 108646.8569062918, 95840.90346684969, 76687.9201149243, 71665.86042137111, 70938.31490697399, 58123.39475566122, 55627.6109897161, 43380.574800812865, 36973.521999485674, 65592.39200216932, 33891.960907159766, 31588.620938388394, 28783.562394131066, 27831.69552051514, 27783.42683587699, 27678.061294768242, 27159.24220492729, 26422.31769685904, 25309.7243490341, 23698.90764066311, 23281.47482143581, 22582.870821767952, 22361.670786301594, 21947.022121095197, 21936.40795117978, 21887.86886309356, 21147.729558476, 20483.869063571365, 110404.16551723496, 60655.73306180881, 41181.38355763391, 34727.48753182077, 88393.94577051663, 32645.240772462195, 53699.13365782846, 57982.050533544956, 41210.760722362844, 48834.63905496174, 51164.06461068674, 32062.104052592833, 64646.82786571342, 76974.72136221887, 42572.08852095918, 56949.696905200086, 35488.99371630737, 41836.84151576323, 36720.76488013493, 46122.34558496438, 47121.235952031646, 26351.54072634509, 3277.417952394272, 2571.0754355857807, 2519.5844965222554, 1008.1849476375249, 966.6823709828996, 940.5442992565529, 787.5929427978996, 742.1262683541453, 701.4610363989988, 698.7774065499481, 686.3706960482721, 670.6234433052639, 658.735868936681, 616.3180378376228, 612.8817276793936, 607.73723248364, 584.1543135506805, 575.0137979268322, 553.4836434734505, 533.364192074188, 488.47198633773417, 487.3928623466107, 471.3427146509816, 469.6590961660524, 467.1393821283865, 463.723197112114, 433.11031163788124, 419.4661593616894, 407.3847821892987, 570.0588491830242, 41002.704261695675, 36523.733692276786, 15743.231987467016, 13316.376651277804, 12694.555849014821, 11611.370393112084, 11583.582247489954, 7761.144708499146, 7494.082838163672, 7300.39149253182, 6462.0791731500185, 6325.447803212393, 6064.567507865551, 5726.556911659583, 5181.251978601222, 4512.766971734033, 4163.833663593035, 4045.8877357787806, 3989.7316100062053, 3979.269971310559, 3600.9527251020295, 3445.371444624504, 3306.8142788839955, 3259.8676419708154, 3203.817968894202, 3103.545517262331, 3101.1200591177762, 3087.77455035559, 2833.998974470173, 2830.1936232542134, 26264.456249019157, 10417.488105325414, 23586.920781744295, 3895.0366143133833, 10163.454714316966, 9843.618254629077, 8439.880444658107, 15073.34646231858, 5888.296480598499, 51786.93720718454, 72666.32947456674, 5888.973785876557, 53703.05379868822, 9229.198576052839, 13579.687394739325, 6674.083043804699, 8374.91169898327, 15068.347177584474, 15052.377942531502, 9132.338598655893, 8023.441490031631, 8214.650491859522, 9701.685540432554, 9825.651035538773, 9715.037369823563, 7929.108433505078, 7804.092383355586, 47621.07637538416, 27962.93502134391, 25697.738426624383, 20614.223841644805, 16432.0139313616, 11746.25541950253, 10112.299052443672, 10032.433462970233, 9415.472821144273, 7960.594681533005, 7628.873225237727, 7304.166447858638, 5530.969602922278, 5373.617318091487, 5332.264721374687, 4546.524600738532, 4460.600313915822, 4298.505448079515, 4030.132805009857, 3743.5208216542287, 3316.2126140870932, 3179.608170703766, 3110.531417308753, 3061.071986306472, 2563.18548452094, 2527.2119258072985, 2519.8811785097114, 2322.129859197524, 2101.9826152484834, 2009.590072813018, 14768.216146930767, 11779.653254200459, 12427.508759463457, 29652.00998143568, 34407.29316863869, 13709.985335368903, 6899.681176940621, 6835.452072662601, 5369.262866984615, 7854.62174007045, 10661.696059927393, 4995.045507022657, 8810.706301387734, 5422.8731796277825, 7671.612444530983, 5483.159119156356, 5722.089188935104, 5590.937227547332, 1061.0306224610958, 979.9740735018539, 666.7649649666329, 552.0855041095941, 486.2891079770929, 460.514059832713, 446.4115776135241, 440.10167681143605, 388.9775747878107, 353.81656068846763, 344.28332370356, 314.36680699281635, 283.714157932698, 281.76769054354276, 262.71550364603485, 262.65757555558764, 261.01030118358335, 228.89964664697933, 220.19398762922688, 218.55018058763633, 216.0139467526376, 213.26954776761607, 212.9827619448374, 204.3389996487286, 177.15842943480902, 161.5979835141998, 154.57240621967608, 149.12497948922888, 148.8038824128776, 147.92932142236455, 1587.8079851445884, 1114.098765008572, 1111.5552660917497, 1064.564867473245, 873.1251769075808, 786.0497167117082, 650.5794894377477, 586.4893583576844, 551.3903673504838, 493.1174216313383, 433.3963407095256, 423.9505544116305, 419.2941356097589, 403.0435274179792, 378.4143078926764, 369.34053891099626, 353.6478948761052, 345.5867160648094, 324.76530621688056, 321.2703339658263, 317.154426198719, 312.58867296632485, 305.6244239108387, 285.55184984935977, 275.12642207326087, 265.7983979866302, 240.7581337386147, 237.1581336676453, 234.91807592721509, 234.49685193374737, 6884.534437634483, 2740.8650305659694, 2167.6054641763994, 1677.116048800955, 1102.7300057508533, 975.3626290357282, 850.1967888225126, 844.7333665114077, 772.8464889185191, 720.8965179780387, 685.4384567564235, 624.9330493393082, 590.6657007770876, 584.1953424074777, 577.271549337064, 571.8319440811534, 567.6267978110166, 557.1441762932631, 533.046710299045, 491.0818295983237, 485.7876835045047, 452.18661706134696, 446.39798687159515, 443.6393414286243, 432.2100269919751, 429.5136561433213, 424.38899641727534, 420.48954905557457, 417.8107607976672, 408.41248076780045, 11138.303885137182, 10410.597957217804, 9424.470859766408, 7779.599952678404, 7528.260599228325, 6634.682957270835, 6282.811995644693, 5960.548421801707, 5399.448046587229, 5188.114208299288, 4670.517754473529, 4201.304352343352, 4014.6652379900734, 3942.9888524599533, 3928.464929686027, 3839.993698748944, 3837.1527059082323, 3676.5711030243797, 3431.2069587825176, 3417.199686504603, 3408.8124018485905, 3239.244589658254, 3233.263726538949, 3183.0435436645275, 3086.129073847902, 2944.7175439155076, 2876.5171439300652, 2857.8777587711143, 2763.910926256075, 2725.9784468388793, 12253.728520607878, 4307.0749915114275, 4473.787383639655, 4276.398429645204, 5091.1098607926015, 3787.1194920246317, 16667.235463922276, 6101.236056152331, 7834.864788980866, 8719.248781583663, 10695.285248042164, 4333.800444339094, 8646.080933300906, 4914.469200793087, 4516.709147148715, 3879.165164544312, 15144.320144058103, 10125.783441435711, 8290.838245847435, 7965.648405551935, 7909.579332491217, 6536.882411571319, 6457.651313874302, 6365.368032271952, 6082.587286301909, 4044.617960896666, 3655.0470331065294, 3302.021520026868, 3061.399888250619, 2948.2100049926066, 2923.209365984557, 2555.1349789563187, 2138.7865925948386, 1998.2090051499524, 1617.2450613468534, 1572.670697939783, 1461.4439814790662, 1439.4157077690395, 1288.4689339593924, 1242.6762675226926, 1208.022946826981, 1086.7517730941054, 1068.185584162724, 1031.7127302117544, 940.2336563999233, 924.5593944819016, 8482.33563074178, 3621.718099445901, 6740.858496539535, 3936.9770063595665, 4886.28460893225, 2974.586059893149, 2710.277410192885, 9702.12313622308, 5303.8501604586145, 2191.4826844392474, 4078.036523287975, 3948.6010824819105, 2724.4434761554826, 2218.1801423662137, 6261.46861222197, 4609.447907269305, 3915.4546515086236, 3310.611342412708, 2975.503095112602, 2630.0428828442514, 2086.851176456317, 1808.729004306053, 1710.7893236590191, 1687.915138524421, 1561.2634706554015, 1457.97920587365, 1443.2096050751045, 1353.2757368984392, 1314.3758967285469, 1258.8733236912212, 1251.4237443410777, 1228.52839380669, 1110.3064076076107, 971.357722632876, 906.3845952533877, 880.9302356436963, 874.4177864380192, 854.6476995952728, 822.3925122077727, 802.1939061912583, 796.7539174135501, 793.2370938258036, 744.1863599856672, 743.328760434349, 6922.632729469799, 2175.6732197682836, 2133.424035253676, 2957.464081422334, 4040.495343834576, 1202.9447969831326, 1438.8150587718462], \"Total\": [119788.0, 99826.0, 47621.0, 78953.0, 26352.0, 51951.0, 39478.0, 30712.0, 41003.0, 27963.0, 36524.0, 174131.0, 25698.0, 52532.0, 73293.0, 20615.0, 127083.0, 30161.0, 26506.0, 15145.0, 15684.0, 92027.0, 23926.0, 72018.0, 28888.0, 108647.0, 110429.0, 18084.0, 16432.0, 18415.0, 174131.56045128297, 127083.85750170558, 108647.71703308079, 95841.76361271036, 76688.78026664339, 71666.72056118725, 70939.17504551983, 58124.2549248431, 55628.47112609501, 43381.43492005881, 36974.38212842624, 65593.9249934633, 33892.82103042196, 31589.481080209738, 28784.422529240066, 27832.555647327023, 27784.286953427058, 27678.92145013032, 27160.102351017078, 26423.177821869034, 25310.584498790395, 23699.767766891186, 23282.33496132184, 22583.730978862204, 22362.530906531876, 21947.882289496054, 21937.268054659828, 21888.728968949017, 21148.589698395095, 20484.729175136843, 110429.73559529461, 60662.87168305294, 41266.454957756505, 34747.268928383484, 89443.39017601806, 32741.369803376125, 54143.912334541325, 58782.76719185049, 41640.092297908326, 50041.18484625936, 52680.00275317411, 32649.24703288016, 73293.6697591556, 92027.84914079026, 45950.49055188155, 72018.79392817474, 38678.3926893108, 52532.887730900475, 46436.552085584124, 99826.14921952719, 119788.31524913845, 26352.409862159333, 3278.2871519505184, 2571.9446124808524, 2520.4536313843637, 1009.0541333619011, 967.551535069874, 941.4135032567159, 788.4622473854624, 742.9954168256446, 702.3301939984156, 699.6467107303647, 687.2398446071078, 671.4926535019389, 659.6050818631732, 617.187220211829, 613.7509293631738, 608.606468063378, 585.0234889404117, 575.8830422096825, 554.3528315115122, 534.2333875686181, 489.3411732760713, 488.262098629972, 472.21188063126965, 470.52830417963213, 468.0085647701325, 464.59233676560495, 433.979542448023, 420.3353604933283, 408.25401230365674, 3492.849196125828, 41003.5582435854, 36524.58770419209, 15744.08601151717, 13317.23061716838, 12695.409838526954, 11612.224419128393, 11584.436222358023, 7761.998715858578, 7494.936832544548, 7301.245519380342, 6462.9331693355225, 6326.301810610371, 6065.421491539411, 5727.4109021905715, 5182.1059751335915, 4513.620970283964, 4164.687680417821, 4046.7416866182652, 3990.58557146562, 3980.123998909206, 3601.8066846356583, 3446.2254554098836, 3307.668250415068, 3260.7216414522873, 3204.671967718519, 3104.3995140316906, 3101.9740219859436, 3088.6285443094157, 2834.8529775716925, 2831.0475842232922, 26506.86578731976, 10439.923981438707, 23926.54406233286, 3904.638355363962, 10405.112737144389, 10386.226601972425, 8838.859857744445, 16760.121040982005, 6147.534631535801, 78953.50406645003, 119788.31524913845, 6443.2545538610575, 99826.14921952719, 11448.938562002826, 27527.510743796, 8925.673043357125, 14406.167099775294, 72018.79392817474, 92027.84914079026, 21967.975498907585, 15684.244976595004, 17610.900288164594, 32316.02736156834, 39478.41162414348, 46436.552085584124, 19343.60864302536, 16353.949348826947, 47621.93726313823, 27963.795879397196, 25698.59930363609, 20615.08474099079, 16432.874842555593, 11747.116284776814, 10113.159895628874, 10033.294298610812, 9416.333743515346, 7961.4555880444295, 7629.734087705985, 7305.027365979307, 5531.83049933092, 5374.4781701861875, 5333.125628156332, 4547.385500714973, 4461.46119428883, 4299.366325650314, 4030.993658494762, 3744.3818160214673, 3317.0735161267685, 3180.469031904897, 3111.39231930194, 3061.932864896865, 2564.0464012094494, 2528.0729740213364, 2520.7420940966517, 2322.990701642366, 2102.843556350325, 2010.4510237894144, 14867.465382306904, 12050.696601792879, 13891.876963443625, 39478.41162414348, 51951.79644186203, 18084.654704378398, 8534.209998240241, 8621.346157235053, 6319.0565558891585, 10724.765136676479, 18415.676620086433, 5890.360206818541, 16819.821683495822, 7845.038572308671, 21967.975498907585, 10200.27065718835, 14406.167099775294, 16970.027374861937, 1061.923153761703, 980.8664298528328, 667.6573030989277, 552.9778788209492, 487.18147901913517, 461.4064417368159, 447.3039548706773, 440.99408394307704, 389.8699850624409, 354.7089593515669, 345.17566793684733, 315.25917984417276, 284.60652324290965, 282.6600570502684, 263.6078078318075, 263.55005124934416, 261.90266571571516, 229.79199109338933, 221.08640421598932, 219.44251066956065, 216.90632580410445, 214.16190441647464, 213.87509433578848, 205.23135856282124, 178.05085632569822, 162.4903558541077, 155.46476352010103, 150.01734772944485, 149.69624024706692, 148.8217052339955, 1588.690029126332, 1114.9807836710875, 1112.4373268154168, 1065.4469016927076, 874.0072875687163, 786.9318239178107, 651.4616455599307, 587.3714419826356, 552.2724474290982, 493.999530211696, 434.2784717584806, 424.832660374561, 420.17625066946033, 403.9255995546491, 379.29640642997504, 370.22261381217106, 354.530030990314, 346.46883322810396, 325.64738071313684, 322.1524241364338, 318.03650479577306, 313.47072084521085, 306.5065151953372, 286.43388244131387, 276.00851100362183, 266.6804633564471, 241.64017139995576, 238.04026343392871, 235.80021034461507, 235.37889486950155, 6885.408597211374, 2741.7392970992437, 2168.4796112425443, 1677.9902980025265, 1103.604233583559, 976.2368306959663, 851.0710255830268, 845.607606406557, 773.7207250250731, 721.7707509277494, 686.3126721625431, 625.8073467287885, 591.5399369140179, 585.0696535465983, 578.1458641459778, 572.7061216351684, 568.501138906019, 558.0184574197311, 533.9209889499249, 491.95600998997367, 486.66192172174857, 453.06084265021406, 447.27220512970166, 444.5135175561396, 433.0842800751642, 430.3879252211942, 425.2632537827071, 421.36381373205535, 418.6850106634393, 409.28669153015977, 11139.169042390622, 10411.463055662494, 9425.335996161464, 7780.465085917123, 7529.125691113757, 6635.548108442395, 6283.677120719306, 5961.413614582421, 5400.3131509877485, 5188.979354786445, 4671.382874834384, 4202.169430537711, 4015.5303821578555, 3943.8539954475573, 3929.330025734027, 3840.8588628188418, 3838.0178357609, 3677.436235582719, 3432.072114431697, 3418.0647896114606, 3409.6775125258573, 3240.1097242861733, 3234.1288474605235, 3183.9086622357, 3086.994237069543, 2945.5827015632485, 2877.3822554278026, 2858.7428958709384, 2764.776146257217, 2726.843577497564, 12269.2742521255, 4430.774605410948, 4713.698916508123, 4651.092571852308, 5773.694312562897, 4069.5186539902525, 30161.644060547216, 10018.844403920028, 17833.335054599283, 28888.83374193827, 52532.887730900475, 7296.8479666048, 73293.6697591556, 29042.720599303073, 26254.770696308475, 28583.037074365122, 15145.170744490595, 10126.634082811683, 8291.688835095723, 7966.499070225835, 7910.429973761549, 6537.733005875937, 6458.501944899411, 6366.218683180154, 6083.437930998614, 4045.4685384696045, 3655.897679395648, 3302.872301527094, 3062.2505199965476, 2949.0606404750747, 2924.059959414508, 2555.985561167327, 2139.6371878510604, 1999.059642664449, 1618.0956293312702, 1573.5214144718093, 1462.2946493078125, 1440.266389743527, 1289.3196402202295, 1243.5268854413016, 1208.8735928434148, 1087.6024539513596, 1069.0362365718129, 1032.563351196543, 941.0842287578026, 925.4099803078968, 9921.906918389006, 4015.4816715532593, 8046.44820363348, 5174.028505764649, 7844.50490756065, 4178.28703247275, 3778.0938011342105, 30712.274806974976, 12016.918649317922, 3075.136944643254, 11001.425442236236, 17135.401761281617, 7443.558870699475, 37684.146519875256, 6262.338471636462, 4610.317767135679, 3916.3245115584546, 3311.4812007526384, 2976.372921714559, 2630.9127884552, 2087.7209951151353, 1809.5988340257838, 1711.6594089827124, 1688.7850091273235, 1562.1333569437095, 1458.8490007991143, 1444.07939097666, 1354.145538201581, 1315.2459089367835, 1259.7432806768043, 1252.2935903869175, 1229.3982381676, 1111.1763354614213, 972.2276774751205, 907.2544211884228, 881.8000820826566, 875.2876574407218, 855.5174965680533, 823.262341920661, 803.0637673319013, 797.623811578666, 794.1069732335393, 745.0563814816901, 744.1985819742232, 11001.425442236236, 3265.4128657332676, 4346.694800205986, 7844.50490756065, 15684.244976595004, 4178.28703247275, 9921.906918389006], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.7177000045776367, -4.032700061798096, -4.1894001960754395, -4.314799785614014, -4.537799835205078, -4.605500221252441, -4.615699768066406, -4.814899921417236, -4.858799934387207, -5.107500076293945, -5.267300128936768, -4.6940999031066895, -5.354300022125244, -5.424699783325195, -5.5177001953125, -5.551300048828125, -5.553100109100342, -5.5569000244140625, -5.575799942016602, -5.603300094604492, -5.646299839019775, -5.712100028991699, -5.729899883270264, -5.760300159454346, -5.770199775695801, -5.788899898529053, -5.789400100708008, -5.791600227355957, -5.826000213623047, -5.857900142669678, -4.173399925231934, -4.772299766540527, -5.1595001220703125, -5.329999923706055, -4.395699977874756, -5.3917999267578125, -4.894100189208984, -4.817399978637695, -5.15880012512207, -4.989099979400635, -4.942500114440918, -5.409800052642822, -4.708600044250488, -4.533999919891357, -5.126299858093262, -4.835299968719482, -5.308300018310547, -5.143700122833252, -5.274199962615967, -5.046199798583984, -5.024799823760986, -1.3271000385284424, -3.411600112915039, -3.654400110244751, -3.6745998859405518, -4.5904998779296875, -4.632599830627441, -4.659999847412109, -4.837399959564209, -4.896900177001953, -4.9532999992370605, -4.957099914550781, -4.974999904632568, -4.998199939727783, -5.01609992980957, -5.082699775695801, -5.0883002281188965, -5.096700191497803, -5.136300086975098, -5.1519999504089355, -5.190199851989746, -5.227200031280518, -5.315100193023682, -5.317399978637695, -5.350800037384033, -5.354400157928467, -5.359799861907959, -5.367099761962891, -5.435400009155273, -5.467400074005127, -5.496699810028076, -5.160699844360352, -3.5508999824523926, -3.666599988937378, -4.508200168609619, -4.675600051879883, -4.723400115966797, -4.812600135803223, -4.815000057220459, -5.215400218963623, -5.250500202178955, -5.276700019836426, -5.398600101470947, -5.420000076293945, -5.462100028991699, -5.519499778747559, -5.619500160217285, -5.757699966430664, -5.838099956512451, -5.8668999671936035, -5.880899906158447, -5.883500099182129, -5.983399868011475, -6.027500152587891, -6.068600177764893, -6.082900047302246, -6.100200176239014, -6.131999969482422, -6.132800102233887, -6.1371002197265625, -6.222899913787842, -6.2241997718811035, -3.9964001178741455, -4.92110013961792, -4.103899955749512, -5.904900074005127, -4.945799827575684, -4.977799892425537, -5.1315999031066895, -4.551700115203857, -5.491600036621094, -3.3173999786376953, -2.9786999225616455, -5.491499900817871, -3.281100034713745, -5.042200088500977, -4.656000137329102, -5.366300106048584, -5.1392998695373535, -4.552000045776367, -4.552999973297119, -5.052800178527832, -5.182199954986572, -5.158699989318848, -4.992300033569336, -4.979599952697754, -4.990900039672852, -5.193999767303467, -5.20989990234375, -2.7464001178741455, -3.2788000106811523, -3.3631999492645264, -3.583699941635132, -3.8104000091552734, -4.146100044250488, -4.295899868011475, -4.303800106048584, -4.367300033569336, -4.535099983215332, -4.577700138092041, -4.621200084686279, -4.8993000984191895, -4.928100109100342, -4.9359002113342285, -5.095300197601318, -5.1143999099731445, -5.151400089263916, -5.215799808502197, -5.289599895477295, -5.410799980163574, -5.452899932861328, -5.474800109863281, -5.490900039672852, -5.668399810791016, -5.682499885559082, -5.685400009155273, -5.767099857330322, -5.866799831390381, -5.9116997718811035, -3.9172000885009766, -4.1433000564575195, -4.089700222015381, -3.220099925994873, -3.0713999271392822, -3.991499900817871, -4.678199768066406, -4.6875, -4.928899765014648, -4.548500061035156, -4.243000030517578, -5.001200199127197, -4.433700084686279, -4.919000148773193, -4.5721001625061035, -4.9079999923706055, -4.865300178527832, -4.888500213623047, -3.33870005607605, -3.4182000160217285, -3.803299903869629, -3.992000102996826, -4.118899822235107, -4.173399925231934, -4.204500198364258, -4.218699932098389, -4.342199802398682, -4.436999797821045, -4.464300155639648, -4.555200099945068, -4.657800197601318, -4.664700031280518, -4.7347002029418945, -4.734899997711182, -4.741199970245361, -4.872499942779541, -4.911200046539307, -4.918700218200684, -4.9303998947143555, -4.94320011138916, -4.944499969482422, -4.986000061035156, -5.128699779510498, -5.220600128173828, -5.265100002288818, -5.301000118255615, -5.303100109100342, -5.309000015258789, -3.327500104904175, -3.68179988861084, -3.6840999126434326, -3.727299928665161, -3.92549991607666, -4.030600070953369, -4.219799995422363, -4.323500156402588, -4.385200023651123, -4.4969000816345215, -4.625999927520752, -4.6479997634887695, -4.65910005569458, -4.698599815368652, -4.761600017547607, -4.785900115966797, -4.8292999267578125, -4.852399826049805, -4.9145002365112305, -4.925300121307373, -4.938199996948242, -4.952700138092041, -4.975299835205078, -5.0432000160217285, -5.080399990081787, -5.1149001121521, -5.213799953460693, -5.228899955749512, -5.238399982452393, -5.240200042724609, -2.3910999298095703, -3.3120999336242676, -3.5467000007629395, -3.803299903869629, -4.222599983215332, -4.345300197601318, -4.482600212097168, -4.489099979400635, -4.578000068664551, -4.647600173950195, -4.6981000900268555, -4.790500164031982, -4.84689998626709, -4.857900142669678, -4.869800090789795, -4.879300117492676, -4.88670015335083, -4.905300140380859, -4.94950008392334, -5.031499862670898, -5.042300224304199, -5.113999843597412, -5.1269001960754395, -5.1331000328063965, -5.159200191497803, -5.165500164031982, -5.177499771118164, -5.186699867248535, -5.1930999755859375, -5.215799808502197, -4.319900035858154, -4.387400150299072, -4.4868998527526855, -4.678699970245361, -4.711599826812744, -4.837900161743164, -4.892399787902832, -4.945099830627441, -5.044000148773193, -5.083899974822998, -5.189000129699707, -5.294899940490723, -5.3403000831604, -5.35830020904541, -5.361999988555908, -5.384799957275391, -5.385499954223633, -5.428299903869629, -5.497300148010254, -5.501399993896484, -5.503900051116943, -5.554900169372559, -5.55679988861084, -5.572400093078613, -5.603300094604492, -5.650199890136719, -5.673699855804443, -5.680200099945068, -5.713600158691406, -5.727399826049805, -4.224400043487549, -5.269999980926514, -5.23199987411499, -5.277100086212158, -5.102799892425537, -5.398600101470947, -3.916800022125244, -4.921800136566162, -4.6717000007629395, -4.564700126647949, -4.360400199890137, -5.263800144195557, -4.5731000900268555, -5.1381001472473145, -5.222499847412109, -5.374599933624268, -3.2118000984191895, -3.614300012588501, -3.814300060272217, -3.854300022125244, -3.861299991607666, -4.0518999099731445, -4.0640997886657715, -4.078499794006348, -4.124000072479248, -4.5320000648498535, -4.633299827575684, -4.734899997711182, -4.810500144958496, -4.848199844360352, -4.8566999435424805, -4.991300106048584, -5.1691999435424805, -5.237199783325195, -5.448699951171875, -5.476600170135498, -5.550000190734863, -5.565199851989746, -5.675899982452393, -5.712100028991699, -5.7403998374938965, -5.846199989318848, -5.863399982452393, -5.898200035095215, -5.991000175476074, -6.007800102233887, -3.7913999557495117, -4.642499923706055, -4.021200180053711, -4.559000015258789, -4.3429999351501465, -4.839300155639648, -4.932400226593018, -3.657099962234497, -4.261000156402588, -5.144800186157227, -4.523799896240234, -4.556000232696533, -4.92710018157959, -5.132699966430664, -3.3141000270843506, -3.6203999519348145, -3.7834999561309814, -3.9512999057769775, -4.05810022354126, -4.18149995803833, -4.412799835205078, -4.555799961090088, -4.611499786376953, -4.625, -4.703000068664551, -4.771399974822998, -4.781599998474121, -4.845900058746338, -4.875100135803223, -4.918300151824951, -4.924200057983398, -4.942699909210205, -5.043799877166748, -5.177499771118164, -5.246799945831299, -5.275199890136719, -5.282700061798096, -5.305500030517578, -5.343999862670898, -5.368899822235107, -5.375699996948242, -5.380099773406982, -5.443900108337402, -5.445099830627441, -3.21370005607605, -4.371099948883057, -4.390699863433838, -4.0640997886657715, -3.7520999908447266, -4.963699817657471, -4.7846999168396], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.4258, 0.4258, 0.4258, 0.4258, 0.4258, 0.4258, 0.4258, 0.4258, 0.4258, 0.4258, 0.4258, 0.4258, 0.4258, 0.4258, 0.4257, 0.4257, 0.4257, 0.4257, 0.4257, 0.4257, 0.4257, 0.4257, 0.4257, 0.4257, 0.4257, 0.4257, 0.4257, 0.4257, 0.4257, 0.4257, 0.4255, 0.4257, 0.4237, 0.4252, 0.414, 0.4228, 0.4175, 0.4121, 0.4154, 0.4014, 0.3966, 0.4076, 0.3002, 0.2472, 0.3494, 0.191, 0.3397, 0.1981, 0.191, -0.3464, -0.5072, 4.7046, 4.7044, 4.7043, 4.7043, 4.7038, 4.7037, 4.7037, 4.7035, 4.7035, 4.7034, 4.7034, 4.7034, 4.7033, 4.7033, 4.7032, 4.7032, 4.7032, 4.7031, 4.7031, 4.7031, 4.703, 4.7028, 4.7028, 4.7028, 4.7028, 4.7028, 4.7027, 4.7026, 4.7026, 4.7025, 2.8919, 2.0387, 2.0387, 2.0387, 2.0387, 2.0387, 2.0386, 2.0386, 2.0386, 2.0386, 2.0386, 2.0386, 2.0386, 2.0386, 2.0386, 2.0386, 2.0385, 2.0385, 2.0385, 2.0385, 2.0385, 2.0385, 2.0385, 2.0385, 2.0385, 2.0385, 2.0384, 2.0384, 2.0384, 2.0384, 2.0384, 2.0295, 2.0366, 2.0244, 2.0363, 2.0152, 1.9851, 1.9925, 1.9326, 1.9956, 1.617, 1.5389, 1.9488, 1.4188, 1.8232, 1.3321, 1.748, 1.4963, 0.4744, 0.2282, 1.161, 1.3684, 1.2761, 0.8355, 0.648, 0.4743, 1.1469, 1.2989, 2.6936, 2.6936, 2.6936, 2.6936, 2.6936, 2.6936, 2.6936, 2.6936, 2.6936, 2.6936, 2.6936, 2.6935, 2.6935, 2.6935, 2.6935, 2.6935, 2.6935, 2.6935, 2.6935, 2.6934, 2.6934, 2.6934, 2.6934, 2.6934, 2.6933, 2.6933, 2.6933, 2.6933, 2.6933, 2.6932, 2.687, 2.6709, 2.5823, 2.4074, 2.2816, 2.4167, 2.4811, 2.4615, 2.5308, 2.3822, 2.1471, 2.5288, 2.0471, 2.3244, 1.6416, 2.0729, 1.7703, 1.5834, 5.9045, 5.9044, 5.904, 5.9037, 5.9035, 5.9034, 5.9033, 5.9033, 5.903, 5.9028, 5.9027, 5.9025, 5.9022, 5.9022, 5.9019, 5.9019, 5.9019, 5.9014, 5.9013, 5.9012, 5.9012, 5.9011, 5.9011, 5.901, 5.9003, 5.8998, 5.8996, 5.8993, 5.8993, 5.8993, 5.5129, 5.5126, 5.5126, 5.5126, 5.5124, 5.5123, 5.5121, 5.5119, 5.5118, 5.5116, 5.5114, 5.5113, 5.5113, 5.5112, 5.5111, 5.511, 5.5109, 5.5109, 5.5107, 5.5107, 5.5107, 5.5106, 5.5105, 5.5103, 5.5102, 5.5101, 5.5098, 5.5097, 5.5097, 5.5097, 4.9828, 4.9826, 4.9825, 4.9824, 4.9821, 4.982, 4.9819, 4.9819, 4.9818, 4.9817, 4.9817, 4.9815, 4.9815, 4.9814, 4.9814, 4.9814, 4.9814, 4.9814, 4.9813, 4.9812, 4.9811, 4.981, 4.981, 4.981, 4.9809, 4.9809, 4.9809, 4.9809, 4.9808, 4.9808, 2.573, 2.573, 2.573, 2.5729, 2.5729, 2.5729, 2.5729, 2.5729, 2.5729, 2.5729, 2.5729, 2.5728, 2.5728, 2.5728, 2.5728, 2.5728, 2.5728, 2.5728, 2.5728, 2.5728, 2.5728, 2.5728, 2.5728, 2.5728, 2.5728, 2.5728, 2.5727, 2.5727, 2.5727, 2.5727, 2.5718, 2.5447, 2.5208, 2.4891, 2.4472, 2.5011, 1.9799, 2.0771, 1.7506, 1.3751, 0.9814, 2.052, 0.4357, 0.7965, 0.813, 0.5759, 3.3738, 3.3738, 3.3738, 3.3738, 3.3738, 3.3738, 3.3738, 3.3738, 3.3738, 3.3737, 3.3737, 3.3736, 3.3736, 3.3736, 3.3736, 3.3736, 3.3735, 3.3735, 3.3734, 3.3734, 3.3733, 3.3733, 3.3732, 3.3732, 3.3732, 3.3731, 3.3731, 3.3731, 3.373, 3.373, 3.2171, 3.2707, 3.1969, 3.1007, 2.9005, 3.0341, 3.0417, 2.2216, 2.556, 3.0351, 2.3815, 1.9061, 2.3688, 0.5413, 4.1547, 4.1546, 4.1546, 4.1546, 4.1545, 4.1545, 4.1544, 4.1543, 4.1543, 4.1543, 4.1543, 4.1542, 4.1542, 4.1542, 4.1542, 4.1541, 4.1541, 4.1541, 4.154, 4.1539, 4.1539, 4.1538, 4.1538, 4.1538, 4.1538, 4.1537, 4.1537, 4.1537, 4.1537, 4.1537, 3.6916, 3.7488, 3.4431, 3.1793, 2.7985, 2.9097, 2.2239]}, \"token.table\": {\"Topic\": [6, 2, 1, 1, 2, 5, 5, 1, 3, 3, 2, 1, 3, 4, 6, 5, 10, 2, 10, 1, 3, 7, 5, 5, 1, 4, 1, 2, 9, 6, 9, 9, 10, 10, 9, 3, 1, 3, 9, 5, 1, 5, 9, 10, 6, 8, 10, 10, 8, 1, 10, 3, 10, 3, 2, 10, 3, 5, 10, 1, 8, 7, 8, 2, 1, 3, 4, 9, 8, 1, 8, 8, 10, 1, 3, 10, 10, 3, 10, 10, 3, 5, 6, 7, 1, 3, 9, 10, 10, 9, 1, 5, 3, 5, 1, 8, 3, 9, 7, 9, 1, 8, 9, 1, 3, 6, 5, 7, 5, 8, 9, 1, 1, 8, 9, 9, 5, 6, 1, 3, 1, 3, 10, 2, 9, 10, 2, 6, 1, 8, 1, 5, 2, 1, 6, 6, 4, 2, 7, 8, 3, 3, 4, 4, 4, 10, 10, 1, 8, 1, 4, 9, 1, 3, 1, 3, 1, 9, 10, 1, 3, 8, 8, 9, 5, 7, 2, 6, 3, 8, 1, 6, 1, 4, 1, 8, 1, 4, 4, 5, 1, 3, 4, 6, 5, 1, 3, 4, 7, 8, 1, 1, 8, 2, 4, 1, 2, 8, 1, 3, 3, 1, 3, 4, 6, 7, 10, 3, 4, 1, 3, 4, 4, 8, 10, 3, 4, 8, 2, 9, 4, 4, 7, 1, 3, 4, 1, 3, 1, 3, 9, 10, 6, 3, 9, 7, 2, 4, 8, 2, 6, 2, 10, 4, 9, 9, 2, 6, 5, 1, 1, 4, 9, 4, 2, 8, 2, 9, 9, 5, 1, 8, 3, 6, 7, 3, 4, 8, 1, 3, 4, 1, 8, 1, 3, 4, 7, 1, 3, 7, 1, 3, 6, 3, 2, 1, 3, 7, 4, 10, 1, 3, 10, 4, 10, 1, 4, 5, 2, 6, 2, 8, 1, 3, 5, 4, 1, 4, 8, 8, 8, 6, 3, 1, 10, 8, 9, 1, 3, 10, 6, 10, 9, 3, 9, 9, 10, 1, 8, 8, 5, 1, 3, 4, 4, 6, 3, 4, 6, 1, 1, 3, 9, 3, 5, 3, 2, 1, 8, 9, 6, 7, 10, 1, 4, 5, 3, 9, 10, 9, 3, 10, 5, 4, 9, 3, 9, 1, 3, 9, 9, 8, 5, 1, 3, 8, 7, 6, 4, 6, 1, 1, 1, 3, 4, 1, 3, 9, 9, 3, 3, 6, 1, 8, 8, 3, 8, 2, 1, 4, 7, 1, 4, 3, 1, 3, 3, 2, 10, 1, 8, 1, 3, 9, 9, 9, 1, 8, 1, 8, 6, 6, 3, 3, 7, 1, 10, 1, 3, 1, 9, 7, 5, 4, 4, 4, 4, 1, 4, 1, 8, 1, 7, 7, 8, 7, 1, 8, 8, 2, 4, 9, 8, 4, 8, 10, 9, 1, 8, 7, 5, 7, 1, 7, 1, 1, 3, 8, 3, 7, 4, 9, 8, 8, 5, 1, 7, 8, 1, 8, 3, 9, 1, 4, 9, 1, 4, 2, 7, 2, 1, 3, 4, 2, 1, 3, 1, 3, 1, 9, 4, 4, 4, 7], \"Freq\": [0.9941418075300845, 0.9968231069311869, 0.9999769069422153, 0.9999676325022252, 0.998466629254624, 0.9991306774332656, 0.9976942722721046, 0.014168364604467963, 0.9858088965356514, 0.9996859993091367, 0.9992663307642018, 0.23502393291802168, 0.4156960208032876, 0.349235640779985, 0.9977084899900633, 0.9978689060391213, 0.999676070653726, 0.9969283527758105, 0.9996546496793286, 0.7907658111686369, 0.2092231649286922, 0.9990872350617025, 0.9953489797344421, 0.9965534434441301, 0.42100000776206137, 0.5789632507105763, 0.9999598007003857, 0.998195906979438, 0.9998841819025043, 0.9956298845459082, 0.9996403463324838, 0.8548759900457925, 0.14503260430038853, 0.998941360228903, 0.9989764832714375, 0.9999677175820106, 0.7591786637627876, 0.18193326990659134, 0.05885764186884767, 0.9960058899956691, 0.9999676044550843, 0.9969822464137628, 0.712014272087805, 0.28791703170474936, 0.998498357856384, 0.999895051202996, 0.9985821455826077, 0.9996147545596678, 0.9998504884683908, 0.9999421963274248, 0.9994099744859088, 0.5091224716065015, 0.49071768275493344, 0.9996991104729448, 0.9990035136082142, 0.9987372531110112, 0.9997786865817943, 0.996594001124487, 0.9992744813119369, 0.02776038299257877, 0.9720647930815997, 0.9980567165141592, 0.9998112625394161, 0.9990756610151292, 0.3632829907767748, 0.38201613777104704, 0.024218866051784985, 0.23045855912891303, 0.9999180382245166, 0.8307417315641898, 0.16919902469873704, 0.9491484456784757, 0.050703280848716066, 0.23080486216594978, 0.5115324334688992, 0.2575833268371373, 0.9995351633730281, 0.3334953479934485, 0.6663782160089476, 0.9996690791270838, 0.9996444064888559, 0.9979834779131425, 0.9973507244418555, 0.9987415555801668, 0.3933689183456389, 0.6066201018761104, 0.6228563889724641, 0.3769517687661843, 0.9993951069731136, 0.9991207253376561, 0.9999594128548254, 0.994478592805374, 0.9998294100127114, 0.9970104896468006, 0.06929566466626041, 0.9305768868479724, 0.947793686508876, 0.052184495945531366, 0.9997303547058519, 0.9995762975071386, 0.07147336819719148, 0.02639782936899755, 0.9020088488161239, 0.7907779184880472, 0.20921019248145145, 0.9976959787962959, 0.9976648379075682, 0.9968562585669489, 0.9965534305912909, 0.999783461697988, 0.9997544571882557, 0.9999757756835519, 0.8642538557302241, 0.13570986140863617, 0.9999456447041468, 0.9992773497174672, 0.9940979990358713, 0.9996068750977022, 0.5900657013196705, 0.409902833867502, 0.9999426620515486, 0.9139791002779913, 0.08598139269044103, 0.9988772106269925, 0.37067923801436714, 0.6292820904299813, 0.9996327244077231, 0.9992913695486494, 0.6981590250464289, 0.30181211460061547, 0.999983435872787, 0.9977685251602492, 0.9989553252624921, 0.9999536805306821, 0.9991203582290824, 0.9995805500095805, 0.9999152258556235, 0.9981060276066979, 0.9967633344686723, 0.999661513355886, 0.9999282569039329, 0.999907592111021, 0.9999110294672253, 0.9998525274416472, 0.9998709996365809, 0.9998546871555457, 0.9985288751306207, 0.9175407128489023, 0.08244913447195326, 0.4623406729581762, 0.5375347561131637, 0.9997021981788883, 0.19381709387143553, 0.8061009280484356, 0.1005959322058222, 0.8993371803904852, 0.9999899745119171, 0.9995916336732146, 0.9990926717983889, 0.5066567816395566, 0.4933246644199598, 0.9997568379923472, 0.9996575041030693, 0.9999373846426663, 0.9940001441717089, 0.9987670436747693, 0.9977428833569032, 0.9986468242360048, 0.9997964972801363, 0.9998671516698618, 0.9999967814491438, 0.9983474570026206, 0.9999667093195094, 0.9996953346340282, 0.9882675491844289, 0.011728088547802634, 0.9264754192761668, 0.07351390506236238, 0.9995988496872354, 0.9975748687706398, 0.5009420322204496, 0.16953419911754306, 0.3294632281078148, 0.9984852265464692, 0.9945746447313285, 0.0450284319929883, 0.9548742864844756, 0.9997756603945694, 0.9980180362482217, 0.9998021778295549, 0.9999554246700766, 0.11812194464747575, 0.8817578008802038, 0.9999844468812729, 0.9999467626593658, 0.438896704071954, 0.16319055533008076, 0.39766961640961784, 0.46202323099304715, 0.5379652567976152, 0.9998750046110488, 0.24070389970044703, 0.09699375854382668, 0.6622870113549205, 0.9965820756326771, 0.9992814558408017, 0.9994180338070292, 0.3086026891627555, 0.6912649249606043, 0.9999762591034934, 0.2488955253202435, 0.7510940481168187, 0.9999427757852328, 0.9999173981661443, 0.9986753638064938, 0.9999310209867771, 0.9998498689844129, 0.999734801711598, 0.998250516501029, 0.9996375042135867, 0.9998593617891026, 0.9999803186684101, 0.9990684946108304, 0.1314179919627962, 0.07562624074116779, 0.7927996249476715, 0.002304950979041716, 0.9975315625963871, 0.004554671372872759, 0.9577823229812431, 0.03741337199145481, 0.9984666589780895, 0.9976651197443199, 0.9767313681974737, 0.023161690419717718, 0.9976584984833194, 0.9990826603981522, 0.9999473812014639, 0.9997192733819453, 0.9980764018227378, 0.9985049757595049, 0.9974151206216634, 0.998389432601385, 0.9995755763253821, 0.9996686321094751, 0.9991146453907731, 0.9978449865108177, 0.9979766575663186, 0.999119124268647, 0.9999721164198887, 0.9997669504943041, 0.00022638829899602916, 0.9996144105106459, 0.9997889365008787, 0.9976912944841739, 0.9996906395715084, 0.9972592265901267, 0.9998878807263499, 0.9994699294399058, 0.9982316131288395, 0.8820270592594324, 0.11796380271871938, 0.9997865780555437, 0.9988158771961064, 0.9988447650389513, 0.9999623441012019, 0.9775368502968861, 0.022405343767415896, 0.5410318478933074, 0.30022254565664996, 0.15874475976279265, 0.5606354599063891, 0.4393457519870533, 0.0874309858680371, 0.06434241484269139, 0.8479956784676609, 0.9994098309127858, 0.9712224245644565, 0.028758540638244696, 0.998639873612041, 0.533476417802099, 0.466472461122325, 0.9963460873001536, 0.999983909354519, 0.999820019944539, 0.0021072950376951134, 0.9978042003486363, 0.9974963762827502, 0.9997056049096059, 0.9991540508983233, 0.5227483476713355, 0.47719360220225787, 0.9998747059846438, 0.9995918945893663, 0.9997141699982001, 0.9896952126129201, 0.0103025708235894, 0.9977458111587607, 0.9995607634102491, 0.9995656615742018, 0.9994299682757113, 0.9996779271377951, 0.3440759257136472, 0.6559176899408322, 0.9950860650167889, 0.999873908764412, 0.0981159124809826, 0.8496521517909607, 0.052064734203618186, 0.9998582547972816, 0.9998012971832766, 0.9967409250820479, 0.9998713326431583, 0.9999898255437017, 0.999052717877077, 0.9997763878211835, 0.9995569744041874, 0.009129710088763754, 0.9908376271540961, 0.999653052560615, 0.9966976252488149, 0.9997862664813594, 0.9997358960784858, 0.9998348782740529, 0.9999373538838786, 0.9990306815275637, 0.999661797291173, 0.001222566200066922, 0.9987550810413375, 0.9996876189089416, 0.9958215796577413, 0.02144914728948408, 0.5813482477327806, 0.3971910057942651, 0.9996763664954819, 0.9972005779298896, 0.10538532725653312, 0.8946235294700776, 0.9980120192838059, 0.9999896650490355, 0.3401598676094123, 0.2938110704825374, 0.3659539807930644, 0.9997942224937502, 0.995908385974037, 0.9999305091097133, 0.9987250397418763, 0.805186997994706, 0.17204492289224632, 0.02273872458859222, 0.9966064052977494, 0.9987330628623338, 0.998967023071229, 0.9999706528697057, 1.5245314258899038e-05, 0.9970848572732735, 0.15547232373099654, 0.8377609386655857, 0.006586757120497855, 0.9994544148832223, 0.9998713071465561, 0.9992179125427169, 0.9991166688690093, 0.9999037859383373, 0.9999169246326746, 0.23888542527798395, 0.7609157923296301, 0.11544201327956907, 0.1716996704552464, 0.7124885946353122, 0.999928012580455, 0.9998922413252149, 0.9979129154149784, 0.23188094472349027, 0.17404775401822262, 0.5939550912716353, 0.9987099117116335, 0.9970561014611113, 0.9998980296240534, 0.9988475066706614, 0.9999920325682502, 0.9999932524734263, 0.9863775179341759, 0.013201147837551663, 0.0004082829228108762, 0.9999800360651726, 0.9998532617694488, 0.9998085703239549, 0.9999222815284886, 0.9997760333337435, 0.9996299658723046, 0.9964227364126685, 0.08041121397225894, 0.9193538795330994, 0.9999306180363984, 0.999717597012175, 0.9999555244387827, 0.9996073705899275, 0.9999934003852049, 0.999858358512806, 0.998171750081184, 0.24186251114548676, 0.7581012866494339, 0.9997903162241601, 0.25219386695721424, 0.7477307277087728, 0.9998945577449936, 0.998660265187237, 0.9986060149692028, 0.9994166756407458, 0.0005468055644649457, 0.22154029096596983, 0.06087726036101919, 0.7172929372972261, 0.9999226984950949, 0.9988478940304485, 0.997056634955872, 0.002901527962040369, 0.39096325305415597, 0.6089524653774331, 0.9974483944272379, 0.9980400273984894, 0.9999863854843503, 0.9998167200489426, 0.997155635617168, 0.9999784096184162, 0.9992525404188962, 0.9999666966067308, 0.9998624230328482, 0.9758961573359004, 0.024100148781552082, 0.9983638997193766, 0.9980012927982904, 0.999885308287336, 0.9999715389355355, 0.9997534953961865, 0.9995735231993544, 0.47610492849979025, 0.5238462194070495, 0.991782781935071, 0.008200367887282288, 0.9999899456654133, 0.9980873554929316, 0.9991184909374441, 0.9997146079451092, 0.998174867862901, 0.9998867234131473, 9.890728601422586e-05, 0.9997401319747882, 0.9994137355504398, 0.993309862861677, 0.006658835077417796, 0.9996884817354262, 0.9998966270760307, 0.9997217079042047, 0.998617343537682, 0.9993228896293831, 0.7963963491653053, 0.20358675226050962, 0.9989321388726841, 0.9990155082616833, 0.9999406575215392, 0.9999644039649922, 0.9997788260308939, 0.9999847708733006, 0.24869333995661214, 0.19869606537261386, 0.5525892410421083, 0.9997979693353515, 0.9982750463664367, 0.8085106883264864, 0.1914647050326781, 0.9999402238925325, 0.9998679172842989, 0.99321846609847, 0.9999915308458874, 0.9970294781609497, 0.999881375079057, 0.9999853209061381, 0.999650957796128, 0.9998556121019555, 0.9994460715409653, 0.6832447329897682, 0.0008465670538378751, 0.31589975216673327, 0.9979291907229738, 0.002035551638394643, 0.9987764916886513, 0.9990986614668735, 0.9975596200928142, 0.01976733264535592, 0.24783759514790582, 0.7324169713644847, 0.9974336083419808, 0.9820134586170162, 0.01794834653950382, 0.8364315880319942, 0.16355918496989383, 0.5585458465578421, 0.44137770711305047, 0.9999766795213619, 0.9999049737186768, 0.9999147954320318, 0.9994524906980493], \"Term\": [\"0\", \"10_min\", \"2\", \"3\", \"60\", \"8_week\", \"aba\", \"able\", \"able\", \"access\", \"ace\", \"advice\", \"advice\", \"advice\", \"advisor\", \"air\", \"alex\", \"alfie\", \"allowance\", \"also\", \"also\", \"alternative\", \"america\", \"answer_soon\", \"anyone\", \"anyone\", \"anything\", \"app\", \"appeal\", \"appeal_decision\", \"application\", \"apply\", \"apply\", \"apply_carers\", \"april\", \"area\", \"ask\", \"ask\", \"ask\", \"asperger_syndrome\", \"autism\", \"avenue\", \"award\", \"award\", \"aww\", \"baby\", \"back_date\", \"backdate\", \"bed\", \"behaviour\", \"ben\", \"benefit\", \"benefit\", \"best_wish\", \"bloody\", \"blue\", \"board\", \"boards\", \"boot\", \"brother\", \"brother\", \"bt\", \"buy\", \"ca\", \"call\", \"call\", \"call\", \"call\", \"calm\", \"cant\", \"cant\", \"car\", \"car\", \"care\", \"care\", \"care\", \"carer\", \"carers\", \"carers\", \"carers_allowance\", \"centre\", \"cerebrum\", \"cf\", \"cheap\", \"child\", \"child\", \"claim\", \"claim\", \"claim_carers\", \"claim_dla\", \"come\", \"comply\", \"condition\", \"conference\", \"constantly\", \"constantly\", \"contact\", \"contact\", \"contact_family\", \"contact_local\", \"copy\", \"copy\", \"copy\", \"could\", \"could\", \"country\", \"county\", \"cream\", \"credit\", \"cry\", \"date\", \"daughter\", \"day\", \"day\", \"decision\", \"department\", \"dependent\", \"dh\", \"diagnosis\", \"diagnosis\", \"didnt\", \"disability\", \"disability\", \"disability_team\", \"dla\", \"dla\", \"doc\", \"document\", \"doesnt\", \"doesnt\", \"dont\", \"download\", \"dry\", \"ds\", \"ds1\", \"ds2\", \"dx\", \"dyspraxic\", \"easter\", \"eat\", \"education\", \"emma\", \"emojifrowning\", \"emojilaughing\", \"emojismile\", \"entitle\", \"equipment\", \"etc\", \"etc\", \"everyone\", \"everyone\", \"evidence\", \"experience\", \"experience\", \"family\", \"family\", \"feel\", \"fill\", \"fill_form\", \"find\", \"find\", \"food\", \"foot\", \"form\", \"foundation\", \"fragile_x\", \"frame\", \"france\", \"funding\", \"game\", \"get\", \"gettin\", \"getting\", \"glad\", \"go\", \"go\", \"going\", \"going\", \"gonna\", \"good_news\", \"great\", \"great\", \"great\", \"grommet\", \"gross_motor\", \"group\", \"group\", \"guy\", \"ha_ha\", \"hands\", \"hard\", \"hate\", \"hate\", \"he\", \"hear\", \"hell\", \"hell\", \"hell\", \"help\", \"help\", \"helpful\", \"hi\", \"hi\", \"hi\", \"hi_kelly\", \"hi_michelle\", \"high_rate\", \"hiya\", \"hiya\", \"home\", \"hope\", \"hope\", \"hopefully\", \"house\", \"hth\", \"hug\", \"hun\", \"hurt\", \"iam\", \"iep\", \"ill\", \"im\", \"im_afraid\", \"im_sure\", \"im_sure\", \"im_sure\", \"important\", \"important\", \"include\", \"include\", \"include\", \"income_support\", \"infection\", \"information\", \"information\", \"instinct\", \"itll\", \"ive\", \"jack\", \"jackson\", \"james\", \"jane\", \"jeanc\", \"josh\", \"july\", \"june\", \"keep_pushing\", \"keep_telling\", \"kelly\", \"kid\", \"know\", \"know\", \"la\", \"lady\", \"laptop\", \"laugh\", \"leaflet\", \"letter\", \"letters\", \"lift\", \"like\", \"like\", \"link\", \"lisa\", \"liz_xx\", \"local\", \"lol\", \"lol\", \"look\", \"look\", \"look\", \"love\", \"love\", \"lovely\", \"lovely\", \"lovely\", \"lucy\", \"make\", \"make\", \"manager\", \"many\", \"many\", \"massage\", \"may\", \"med\", \"meet\", \"meet\", \"member_staff\", \"message\", \"middle_rate\", \"might\", \"might\", \"mobility\", \"monday\", \"money\", \"much\", \"much\", \"much_longer\", \"muscle\", \"n\", \"na\", \"nappy\", \"need\", \"need\", \"needle\", \"next_week\", \"nice\", \"nice\", \"nice\", \"night\", \"noise\", \"oct\", \"offer\", \"one\", \"oops\", \"pain\", \"paperwork\", \"parent\", \"parent\", \"parent_partnership\", \"patch\", \"pay\", \"payment\", \"perhaps\", \"phone\", \"phone_call\", \"physio\", \"play\", \"play\", \"playing\", \"playschool\", \"please\", \"please\", \"please\", \"pm\", \"po\", \"post\", \"post\", \"prescription\", \"problem\", \"process\", \"process\", \"process\", \"professional\", \"project\", \"provide\", \"pull_up\", \"put\", \"put\", \"put\", \"qualification\", \"r\", \"rate\", \"really\", \"really\", \"rebecca\", \"receive\", \"receive\", \"receive\", \"recieved\", \"recommend\", \"referal\", \"register\", \"reply\", \"report\", \"request\", \"request\", \"review\", \"review\", \"review\", \"ring\", \"room\", \"roughly\", \"run\", \"run\", \"run\", \"ruth\", \"sara\", \"sarah\", \"save\", \"say\", \"school\", \"see\", \"see\", \"see\", \"seem\", \"sen\", \"send\", \"sent\", \"service\", \"services\", \"shirt\", \"sister\", \"sister\", \"sit\", \"site\", \"sleep\", \"sn\", \"son\", \"soon\", \"sore\", \"sorry\", \"sorry\", \"sorry_hear\", \"sound_like\", \"sound_like\", \"special_need\", \"spelling\", \"ss\", \"start\", \"start\", \"state\", \"state\", \"state\", \"statement\", \"statutory_assessment\", \"still\", \"still\", \"stop\", \"stop\", \"sum\", \"sunday\", \"support\", \"support_group\", \"symbol\", \"take\", \"tax_credits\", \"teacher\", \"team\", \"tell\", \"tell\", \"tesco\", \"text\", \"thank\", \"thanks\", \"thanks_reply\", \"thankyou\", \"thats\", \"thats\", \"things\", \"things\", \"think\", \"thomas\", \"threaten\", \"throw\", \"tie\", \"time\", \"time\", \"tire\", \"tj\", \"today\", \"today\", \"toilet\", \"tomorrow\", \"toy\", \"transport\", \"tribunal\", \"try\", \"try\", \"tube\", \"twins\", \"u\", \"understand\", \"ur\", \"us\", \"use\", \"use\", \"use\", \"useful\", \"visiting\", \"waiting\", \"waiting\", \"walk\", \"walking\", \"wandering\", \"want\", \"ward\", \"watch\", \"way\", \"wear\", \"website\", \"wednesday\", \"week\", \"week\", \"week\", \"well\", \"well\", \"wen\", \"whats_best\", \"wipe\", \"wonder\", \"wonder\", \"wonder\", \"wont_eat\", \"work\", \"work\", \"would\", \"would\", \"write\", \"write\", \"x\", \"xx\", \"xxx\", \"xxxx\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1515641399381230276164322958571\", ldavis_el1515641399381230276164322958571_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1515641399381230276164322958571\", ldavis_el1515641399381230276164322958571_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1515641399381230276164322958571\", ldavis_el1515641399381230276164322958571_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "pyLDAvis.display(pyLDAvis.gensim.prepare(mod_dict[str(n)], corpus, dictionary, sort_topics=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ldamodel = LdaModel.load(path_ntopic_models.format(f, group, id_type, str(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find dominant topics for each message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data used to create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraping import create_connection\n",
    "from lemmatize import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df of clean text from csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each n topic, find dominant topic in each message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in n_topics:\n",
    "    ldamodel = LdaModel.load(path_tune_models.format(forum, group, str(n)))\n",
    "    topic_sentences = format_topics_sentences(ldamodel, corpus)\n",
    "    df_joined = pd.concat([df.reset_index(drop=True), topic_sentences.reset_index(drop=True)], axis=1)\n",
    "    df_joined[[\"message_id\",\"text_clean\",\"Dominant_Topic\",\"Perc_Contribution\"]].to_csv(path_dom_topic.format(forum, group, str(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dominant = pd.DataFrame()\n",
    "topic_grouped = df_joined.groupby('Dominant_Topic')\n",
    "for i, grp in topic_grouped:\n",
    "    topic_dominant = pd.concat([topic_dominant,\n",
    "                                grp.sort_values(['Perc_Contribution'],\n",
    "                                                ascending=[0]).head(3)],\n",
    "                               axis=0)\n",
    "topic_dominant.reset_index(drop=True, inplace=True)\n",
    "topic_dominant.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dominant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in topic_dominant['text_clean']:\n",
    "    print(t)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot differences between topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_difference_plotly(mdiff, title=\"\", annotation=None):\n",
    "    \"\"\"Plot the difference between models.\n",
    "\n",
    "    Uses plotly as the backend.\"\"\"\n",
    "    import plotly.graph_objs as go\n",
    "    import plotly.offline as py\n",
    "\n",
    "    annotation_html = None\n",
    "    if annotation is not None:\n",
    "        annotation_html = [\n",
    "            [\n",
    "                \"+++ {}<br>--- {}\".format(\", \".join(int_tokens), \", \".join(diff_tokens))\n",
    "                for (int_tokens, diff_tokens) in row\n",
    "            ]\n",
    "            for row in annotation\n",
    "        ]\n",
    "\n",
    "    data = go.Heatmap(z=mdiff, colorscale='RdBu', text=annotation_html)\n",
    "    layout = go.Layout(width=950, height=950, title=title, xaxis=dict(title=\"topic\"), yaxis=dict(title=\"topic\"))\n",
    "    py.iplot(dict(data=[data], layout=layout))\n",
    "\n",
    "\n",
    "def plot_difference_matplotlib(mdiff, title=\"\", annotation=None):\n",
    "    \"\"\"Helper function to plot difference between models.\n",
    "\n",
    "    Uses matplotlib as the backend.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(18, 14))\n",
    "    data = ax.imshow(mdiff, cmap='RdBu_r', origin='lower')\n",
    "    plt.title(title)\n",
    "    plt.colorbar(data)\n",
    "\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "    import plotly.offline as py\n",
    "except Exception:\n",
    "    #\n",
    "    # Fall back to matplotlib if we're not in a notebook, or if plotly is\n",
    "    # unavailable for whatever reason.\n",
    "    #\n",
    "    plot_difference = plot_difference_matplotlib\n",
    "else:\n",
    "    py.init_notebook_mode()\n",
    "    plot_difference = plot_difference_plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel5 = LdaModel.load(path_ntopic_models.format(forum, group, id_type, str(5)))\n",
    "ldamodel10 = LdaModel.load(path_ntopic_models.format(forum, group, id_type, str(10)))\n",
    "ldamodel15 = LdaModel.load(path_ntopic_models.format(forum, group, id_type, str(15)))\n",
    "ldamodel20 = LdaModel.load(path_ntopic_models.format(forum, group, id_type, str(20)))\n",
    "ldamodel25 = LdaModel.load(path_ntopic_models.format(forum, group, id_type, str(25)))\n",
    "ldamodel30 = LdaModel.load(path_ntopic_models.format(forum, group, id_type, str(30)))\n",
    "ldamodel40 = LdaModel.load(path_ntopic_models.format(forum, group, id_type, str(40)))\n",
    "ldamodel50 = LdaModel.load(path_ntopic_models.format(forum, group, id_type, str(50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdiff, annotation = ldamodel10.diff(ldamodel5, distance=\"hellinger\", num_words=50)\n",
    "plot_difference(mdiff, title=\"topic difference\", annotation=annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the topics. See: https://www.objectorientedsubject.net/2018/08/experiments-on-topic-modeling-pyldavis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Dominant Topic in each Post?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamodel, corpus=corpus)\n",
    "df_topic_sents_keywords.info()\n",
    "df_topic_sents_keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "path_db = str(path_parent / \"database\" / \"youbemom-merged.db\")\n",
    "sql = '''\n",
    "    SELECT s.text_no_url AS text_no_url, s.text as text\n",
    "    FROM sentiment AS s\n",
    "    JOIN posts AS p\n",
    "    ON s.message_id = p.message_id\n",
    "    WHERE p.subforum=\"special-needs\" AND p.parent_id=\"\"\n",
    "'''\n",
    "conn = create_connection(path_db)\n",
    "sn = pd.read_sql_query(sql, conn)\n",
    "sn.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model Topics and Keywords in New Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(path_db)\n",
    "df_topic_sents_keywords.to_sql('topicmodel', conn, if_exists='replace', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
