{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Word Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel, LdaModel, LdaMulticore\n",
    "from gensim.models.callbacks import PerplexityMetric, ConvergenceMetric, CoherenceMetric\n",
    "# Managing data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# DB connection\n",
    "from scraping import create_connection\n",
    "# Files & I/O\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from io import FileIO\n",
    "# For logging\n",
    "import logging\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# Random\n",
    "import random\n",
    "# Parallelizing\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path.cwd()\n",
    "path_parent = p.parents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database\n",
    "path_db = str(path_parent / \"database\" / \"netmums-merged.db\")\n",
    "path_clean_data = path_parent / \"clean_data\" / \"netmums\"\n",
    "# data to load\n",
    "path_lemma_pkl = str(path_clean_data / \"lemmatized_text_{0}_{1}_{2}.pkl\")\n",
    "path_corpus_pkl = str(path_clean_data / \"corpus_{0}_{1}_{2}.pkl\")\n",
    "path_dictionary_gensim = str(path_clean_data / \"dictionary_{0}_{1}_{2}.gensim\")\n",
    "# model saving\n",
    "path_tune_models = str(path_clean_data / \"lda_tune_{0}_{1}_{2}_{3}_{4}.gensim\")\n",
    "path_ntopic_models = str(path_clean_data / \"lda_ntopics_{0}_{1}_{2}_{3}.gensim\")\n",
    "# path_coherence = str(path_parent / \"clean_data\" / \"coherence_{}.csv\")\n",
    "path_log = str(path_clean_data / \"logging_{0}_{1}_{2}_{3}.log\")\n",
    "path_log_iterations = str(path_clean_data / \"logging_{0}_{1}_{2}_{3}.log\")\n",
    "# dominant topic\n",
    "path_dom_topic = str(path_clean_data / \"dominant_topic_{0}_{1}_{2}_{3}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes\n",
    "path_topics_pkl = str(path_clean_data / \"daily_topics.pkl\")\n",
    "path_text_pkl = str(path_clean_data / \"daily_clean_text.pkl\")\n",
    "path_days_since_pkl = str(path_clean_data / \"daily_days_since.pkl\")\n",
    "path_subforums_pkl = str(path_clean_data / \"daily_subforums.pkl\")\n",
    "path_emote_pkl = str(path_clean_data / \"daily_emote_processed_{}.pkl\")\n",
    "path_joined_pkl = str(path_clean_data / \"daily_joined_df.pkl\")\n",
    "path_bigrams_pkl = str(path_corpus_pkl.format(\"all\", \"all\", \"daily_text_df\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process all text for topics\n",
    "Make lemmatized text, dictionary, and corpus for all text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import lemmatize_all as la\n",
    "# la.process_data(chunksize=1000000, n_chunks=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make corpuses for all text chunk. The prior command didn't create individual corpuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_chunks = 4\n",
    "# for i in tqdm(range(16)):\n",
    "#     text = pickle.load(open(path_lemma_pkl.format(\"all\", \"all\", \"thread_id_{}\".format(i)), 'rb'))\n",
    "#     corpus = la.text_to_corpus(text, n_chunks, dictionary)\n",
    "#     pickle.dump(corpus, open(path_corpus_pkl.format(\"all\", \"all\", \"thread_id_{}\".format(i)), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chunks = 10\n",
    "for i in tqdm(range(16)):\n",
    "    text = pickle.load(open(path_lemma_pkl.format(\"all\", \"all\", \"message_id_{}\".format(i)), 'rb'))\n",
    "    corpus = la.text_to_corpus(text, n_chunks, dictionary)\n",
    "    pickle.dump(corpus, open(path_corpus_pkl.format(\"all\", \"all\", \"message_id_{}\".format(i)), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make single corpus using the whole dictionary. The process_data command didn't create a good corpus using the incremental approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_chunks = 4\n",
    "# corpus = []\n",
    "# for i in tqdm(range(16)):\n",
    "#     text = pickle.load(open(path_lemma_pkl.format(\"all\", \"all\", \"thread_id_{}\".format(i)), 'rb'))\n",
    "#     corpus = corpus + la.text_to_corpus(text, n_chunks, dictionary)\n",
    "# pickle.dump(corpus, open(path_corpus_pkl.format(\"all\", \"all\", \"thread_id_v2\"), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make 10% sample lemmatized text and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import lemmatize_all as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary.load(path_dictionary_gensim.format(\"all\", \"all\", \"thread_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_text = []\n",
    "corpus = []\n",
    "n_chunks = 1\n",
    "for i in tqdm(range(16)):\n",
    "    text = pickle.load(open(path_lemma_pkl.format(\"all\", \"all\", \"thread_id_{}\".format(i)), 'rb'))\n",
    "    list_len = len(text)\n",
    "    list_10p = int(list_len * .1)\n",
    "    text = random.sample(text, list_10p)\n",
    "    lemmatized_text = lemmatized_text + text\n",
    "    corpus = corpus + la.text_to_corpus(text, n_chunks, dictionary)\n",
    "pickle.dump(lemmatized_text, open(path_lemma_pkl.format(\"all\", \"all\", \"thread_id_10p\"), 'wb'))\n",
    "pickle.dump(corpus, open(path_corpus_pkl.format(\"all\", \"all\", \"thread_id_10p\"), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Topic Model for 10% sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary.load(path_dictionary_gensim.format(\"all\", \"all\", \"thread_id\"))\n",
    "corpus = pickle.load(open(path_corpus_pkl.format(\"all\", \"all\", \"thread_id_10p\"), 'rb'))\n",
    "lemmatized_text = pickle.load(open(path_lemma_pkl.format(\"all\", \"all\", \"thread_id_10p\".format(2)), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_topics = 20\n",
    "lda = LdaModel(\n",
    "    corpus=corpus,\n",
    "    num_topics=n_topics,\n",
    "    id2word=dictionary,\n",
    "    random_state=1,\n",
    "    alpha=\"auto\",\n",
    "    eta=\"auto\"\n",
    ")\n",
    "coherence_model_lda = CoherenceModel(model=lda, texts=lemmatized_text, dictionary=dictionary, coherence='c_v')\n",
    "print(coherence_model_lda.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.save(path_ntopic_models.format(\"all\", \"all\", \"thread_id_10p\", str(n_topics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily text to corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lemmatize_all as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary.load(path_dictionary_gensim.format(\"all\", \"all\", \"thread_id\"))\n",
    "clean_text = pd.read_pickle(path_text_pkl)\n",
    "clean_text.columns = ['user_url', 'day', 'text_clean', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_bigrams, corpus = la.make_corpus(clean_text, dictionary, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bigrams.to_pickle(path_corpus_pkl.format(\"all\", \"all\", \"daily_text_df\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(corpus, open(path_corpus_pkl.format(\"all\", \"all\", \"daily_text\"), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = pickle.load(open(path_corpus_pkl.format(\"all\", \"all\", \"daily_text\"), 'rb'))\n",
    "# len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily text to topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 20\n",
    "lda = LdaModel.load(path_ntopic_models.format(\"all\", \"all\", \"thread_id_10p\", str(n_topics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import corpus2csc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_topics = lda.get_document_topics(corpus, minimum_probability=0.0)\n",
    "all_topics_csr = corpus2csc(all_topics)\n",
    "all_topics_numpy = all_topics_csr.T.toarray()\n",
    "all_topics_df = pd.DataFrame(all_topics_numpy)\n",
    "all_topics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"topic_{}\".format(i) for i in range(20)]\n",
    "all_topics_df.columns = column_names\n",
    "all_topics_df.to_pickle(path_topics_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "    SELECT\n",
    "        text.text_clean AS text_clean,\n",
    "        s.name AS subforum_name,\n",
    "        p.user_url AS user_url,\n",
    "        p.date_created AS date_created\n",
    "    FROM text\n",
    "    LEFT JOIN posts AS p\n",
    "    ON text.post_id = p.id\n",
    "    LEFT JOIN threads AS t\n",
    "    ON t.id=p.thread_id\n",
    "    LEFT JOIN subforums AS s\n",
    "    ON s.id=t.subforum_id\n",
    "    LEFT JOIN forums AS f\n",
    "    ON f.id=s.forum_id\n",
    "    WHERE text.text_clean<>\"\"\n",
    "    AND p.user_url<>\"Anonymous\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = create_connection(path_db)\n",
    "df = pd.read_sql_query(sql, conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create days dataframe\n",
    "ddf = dd.from_pandas(df, npartitions=200)\n",
    "ddf['date_created'] = dd.to_datetime(ddf['date_created'])\n",
    "ddf['day'] = ddf['date_created'].dt.date\n",
    "df = ddf.compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unique days in dataset\n",
    "ddf = dd.from_pandas(df, npartitions=200)\n",
    "ddf = ddf[['user_url', 'day']].groupby([\"user_url\"])[\"day\"].nunique().reset_index(drop=False)\n",
    "df_count = ddf.compute(scheduler='processes')\n",
    "df_count.columns = ['user_url','n_unique_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count posts per day\n",
    "ddf = dd.from_pandas(df, npartitions=200)\n",
    "ddf = ddf.groupby([\"user_url\", \"day\"])['subforum_name'].count().reset_index(drop=False)\n",
    "df_daily_count = ddf.compute(scheduler='processes')\n",
    "df_daily_count.columns = ['user_url', 'day', 'n_posts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily_count['day'] = df_daily_count['day'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "df_daily_count.to_stata(path_clean_data / \"daily_panel_counts.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morethanone = df_count.loc[df_count['n_unique_days'] > 1, 'user_url']\n",
    "df = df.loc[df['user_url'].isin(morethanone)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days since last post\n",
    "ddf = dd.from_pandas(df, npartitions=200)\n",
    "ddf = ddf[['user_url', 'day']].groupby([\"user_url\", \"day\"])['day'].count().to_frame().rename(columns={'day':'daily_count'}).reset_index(drop=False).sort_values(['user_url', 'day'])\n",
    "df_days_since = ddf.compute(scheduler='processes')\n",
    "df_days_since['datediff'] = df_days_since[['user_url', 'day', 'daily_count']].groupby(['user_url'])['day'].diff()\n",
    "df_days_since['days_since_last_post'] = 0\n",
    "df_days_since.loc[df_days_since['datediff'].notna(), 'days_since_last_post'] = df_days_since.loc[df_days_since['datediff'].notna(), 'datediff'].apply(lambda x: x.days)\n",
    "df_days_since = df_days_since.drop(\"datediff\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_days_since.to_pickle(path_days_since_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_days_since.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forum posted in\n",
    "df_subforums = df[['user_url', 'day', 'subforum_name']].groupby([\"user_url\", \"day\", \"subforum_name\"])[\"subforum_name\"].count().reset_index(name=\"count\")\n",
    "df_subforums = df_subforums.pivot(index=['user_url', 'day'], columns='subforum_name', values='count').reset_index(drop=False).fillna(0).reset_index(drop=True)\n",
    "df_subforums.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subforums.to_pickle(path_subforums_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Join Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_days_since = pd.read_pickle(path_days_since_pkl)\n",
    "df_subforums = pd.read_pickle(path_subforums_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emote_0 = pd.read_pickle(path_emote_pkl.format(0))\n",
    "df_emote_1 = pd.read_pickle(path_emote_pkl.format(1))\n",
    "df_emote_2 = pd.read_pickle(path_emote_pkl.format(2))\n",
    "df_emote_0 = df_emote_0.drop(\"scores\", axis=1)\n",
    "df_emote_1 = df_emote_1.drop(\"scores\", axis=1)\n",
    "df_emote_2 = df_emote_2.drop(\"scores\", axis=1)\n",
    "df_emote = pd.concat([df_emote_0, df_emote_1, df_emote_2], axis=0).reset_index(drop=True)\n",
    "df_emote['day'] = df_emote['day'].apply(lambda x: x.date())\n",
    "df_emote = df_emote.groupby(['user_url', 'day']).agg(anger=(\"anger\", np.mean),\n",
    "                                          joy=(\"joy\", np.mean),\n",
    "                                          optimism=(\"optimism\", np.mean),\n",
    "                                          sadness=(\"sadness\", np.mean)).reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_url</th>\n",
       "      <th>day</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>2015-10-21</td>\n",
       "      <td>Hi I didn't get into carrying until ds 2 was a...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.774, 'pos': 0.226, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150</td>\n",
       "      <td>2016-05-20</td>\n",
       "      <td>I have low iron in normal life although convin...</td>\n",
       "      <td>{'neg': 0.16, 'neu': 0.795, 'pos': 0.045, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1st-time-mummy</td>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>Hi Helen I just wondered how your little boy i...</td>\n",
       "      <td>{'neg': 0.119, 'neu': 0.833, 'pos': 0.048, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24h</td>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>Did you get your positive opk Siobhan? Hi Clai...</td>\n",
       "      <td>{'neg': 0.092, 'neu': 0.68, 'pos': 0.228, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2557</td>\n",
       "      <td>2015-10-19</td>\n",
       "      <td>Sian Are you alright now? Hope everything is o...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.507, 'pos': 0.493, 'comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_url        day  \\\n",
       "0             150 2015-10-21   \n",
       "1             150 2016-05-20   \n",
       "2  1st-time-mummy 2018-10-30   \n",
       "3             24h 2018-12-30   \n",
       "4            2557 2015-10-19   \n",
       "\n",
       "                                                text  \\\n",
       "0  Hi I didn't get into carrying until ds 2 was a...   \n",
       "1  I have low iron in normal life although convin...   \n",
       "2  Hi Helen I just wondered how your little boy i...   \n",
       "3  Did you get your positive opk Siobhan? Hi Clai...   \n",
       "4  Sian Are you alright now? Hope everything is o...   \n",
       "\n",
       "                                           sentiment  \n",
       "0  {'neg': 0.0, 'neu': 0.774, 'pos': 0.226, 'comp...  \n",
       "1  {'neg': 0.16, 'neu': 0.795, 'pos': 0.045, 'com...  \n",
       "2  {'neg': 0.119, 'neu': 0.833, 'pos': 0.048, 'co...  \n",
       "3  {'neg': 0.092, 'neu': 0.68, 'pos': 0.228, 'com...  \n",
       "4  {'neg': 0.0, 'neu': 0.507, 'pos': 0.493, 'comp...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment = pd.read_pickle(path_text_pkl)\n",
    "df_sentiment = df_sentiment.drop(\"text\", axis=1)\n",
    "df_sentiment[['neg', 'neu', 'pos','compound']] = df_sentiment['sentiment'].apply(pd.Series)\n",
    "df_sentiment = df_sentiment.drop(\"sentiment\", axis=1)\n",
    "df_sentiment['day'] = df_sentiment['day'].apply(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bigrams = pd.read_pickle(path_bigrams_pkl)\n",
    "df_topics = pd.read_pickle(path_topics_pkl)\n",
    "df_topics = pd.concat([df_bigrams.reset_index(drop=True), df_topics.reset_index(drop=True)], axis=1).drop('bigrams', axis=1)\n",
    "df_topics['day'] = df_topics['day'].apply(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_sentiment.merge(df_emote, how=\"inner\", on=[\"user_url\",\"day\"])\n",
    "df = df.merge(df_subforums, how=\"inner\", on=[\"user_url\",\"day\"])\n",
    "df = df.merge(df_days_since, how=\"inner\", on=[\"user_url\",\"day\"])\n",
    "df = df.merge(df_topics, how=\"inner\", on=[\"user_url\",\"day\"])\n",
    "df.to_pickle(path_joined_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(path_joined_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(path_joined_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_sql = '''\n",
    "    SELECT\n",
    "        p.id AS post_id,\n",
    "        p.user_url,\n",
    "        f.id AS forum_id\n",
    "    FROM posts AS p\n",
    "    LEFT JOIN threads AS t\n",
    "    ON t.id=p.thread_id\n",
    "    LEFT JOIN subforums AS s\n",
    "    ON s.id=t.subforum_id\n",
    "    LEFT JOIN forums AS f\n",
    "    ON f.id=s.forum_id\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = create_connection(path_db)\n",
    "sn_users = pd.read_sql_query(posts_sql, conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_users = sn_users.loc[sn_users['forum_id']==24]\n",
    "sn_users = sn_users.drop_duplicates('user_url')[['user_url']]\n",
    "sn_users = sn_users.loc[sn_users['user_url']!=\"Anonymous\"]\n",
    "sn_users.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sn_user'] = 0\n",
    "df.loc[df['user_url'].isin(sn_users['user_url']), 'sn_user'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_id'] = df.groupby('user_url').ngroup()\n",
    "df['time_period'] = df.sort_values(['day']).groupby(['day']).ngroup()\n",
    "df['first_period'] = df.groupby(['user_url'])['time_period'].transform('min')\n",
    "df['last_period'] = df.groupby(['user_url'])['time_period'].transform('max')\n",
    "df['time_since_first_period'] = df['time_period'] - df['first_period']\n",
    "df['is_last_period'] = 0\n",
    "df.loc[df['time_period']==df['last_period'], 'is_last_period'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(path_joined_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sn_post_sql = '''\n",
    "WITH added_row_number AS (\n",
    "    SELECT\n",
    "        p.id AS post_id,\n",
    "        p.user_url,\n",
    "        p.date_created,\n",
    "        ROW_NUMBER() OVER(PARTITION BY p.user_url ORDER BY p.date_created ASC) AS row_number\n",
    "    FROM posts AS p\n",
    "    LEFT JOIN threads AS t\n",
    "    ON t.id=p.thread_id\n",
    "    LEFT JOIN subforums AS s\n",
    "    ON s.id=t.subforum_id\n",
    "    WHERE s.forum_id=24\n",
    ")\n",
    "SELECT\n",
    "  *\n",
    "FROM added_row_number\n",
    "WHERE row_number = 1;\n",
    "'''\n",
    "conn = create_connection(path_db)\n",
    "first_sn_post = pd.read_sql_query(first_sn_post_sql, conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sn_post['day'] = pd.to_datetime(first_sn_post['date_created']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sn_post = first_sn_post.rename(columns={'day':'first_sn_day'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sn_post = first_sn_post[['user_url', 'first_sn_day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sn_post.to_csv(path_clean_data / \"first_sn_day.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Panel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_url',\n",
       " 'day',\n",
       " 'anger',\n",
       " 'joy',\n",
       " 'optimism',\n",
       " 'sadness',\n",
       " 'sn_user',\n",
       " 'user_id',\n",
       " 'time_period',\n",
       " 'first_period',\n",
       " 'last_period',\n",
       " 'time_since_first_period',\n",
       " 'is_last_period']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_panel = [all_cols[i] for i in [0, 1, 6, 7, 8, 9, 289, 290, 291, 292, 293, 294, 295]]\n",
    "cols_panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_panel = df[cols_panel].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_panel['year'] = pd.DatetimeIndex(df_panel['day']).year\n",
    "df_panel['month'] = pd.DatetimeIndex(df_panel['day']).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_url</th>\n",
       "      <th>day</th>\n",
       "      <th>anger</th>\n",
       "      <th>joy</th>\n",
       "      <th>optimism</th>\n",
       "      <th>sadness</th>\n",
       "      <th>sn_user</th>\n",
       "      <th>user_id</th>\n",
       "      <th>time_period</th>\n",
       "      <th>first_period</th>\n",
       "      <th>last_period</th>\n",
       "      <th>time_since_first_period</th>\n",
       "      <th>is_last_period</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>546615</th>\n",
       "      <td>0407nc</td>\n",
       "      <td>2017-02-25</td>\n",
       "      <td>0.027597</td>\n",
       "      <td>0.010135</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.948204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4283</td>\n",
       "      <td>4283</td>\n",
       "      <td>4364</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365103</th>\n",
       "      <td>0407nc</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>0.031126</td>\n",
       "      <td>0.009913</td>\n",
       "      <td>0.006925</td>\n",
       "      <td>0.952036</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4286</td>\n",
       "      <td>4283</td>\n",
       "      <td>4364</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554764</th>\n",
       "      <td>0407nc</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>0.010955</td>\n",
       "      <td>0.012001</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>0.970060</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4287</td>\n",
       "      <td>4283</td>\n",
       "      <td>4364</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976701</th>\n",
       "      <td>0407nc</td>\n",
       "      <td>2017-03-02</td>\n",
       "      <td>0.023654</td>\n",
       "      <td>0.808639</td>\n",
       "      <td>0.116215</td>\n",
       "      <td>0.051492</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4288</td>\n",
       "      <td>4283</td>\n",
       "      <td>4364</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812262</th>\n",
       "      <td>0407nc</td>\n",
       "      <td>2017-03-14</td>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.397049</td>\n",
       "      <td>0.031483</td>\n",
       "      <td>0.553821</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4300</td>\n",
       "      <td>4283</td>\n",
       "      <td>4364</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_url         day     anger       joy  optimism   sadness  sn_user  \\\n",
       "546615    0407nc  2017-02-25  0.027597  0.010135  0.014064  0.948204        0   \n",
       "365103    0407nc  2017-02-28  0.031126  0.009913  0.006925  0.952036        0   \n",
       "1554764   0407nc  2017-03-01  0.010955  0.012001  0.006985  0.970060        0   \n",
       "976701    0407nc  2017-03-02  0.023654  0.808639  0.116215  0.051492        0   \n",
       "1812262   0407nc  2017-03-14  0.017647  0.397049  0.031483  0.553821        0   \n",
       "\n",
       "         user_id  time_period  first_period  last_period  \\\n",
       "546615         0         4283          4283         4364   \n",
       "365103         0         4286          4283         4364   \n",
       "1554764        0         4287          4283         4364   \n",
       "976701         0         4288          4283         4364   \n",
       "1812262        0         4300          4283         4364   \n",
       "\n",
       "         time_since_first_period  is_last_period  year  month  \n",
       "546615                         0               0  2017      2  \n",
       "365103                         3               0  2017      2  \n",
       "1554764                        4               0  2017      3  \n",
       "976701                         5               0  2017      3  \n",
       "1812262                       17               0  2017      3  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_panel.sort_values(['user_url', 'day']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_panel['day'] = df_panel['day'].apply(lambda x: x.strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_panel.to_stata(path_clean_data / \"daily_panel.dta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cox Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path_clean_data / \"daily_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cox = df[cols_cox].copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
