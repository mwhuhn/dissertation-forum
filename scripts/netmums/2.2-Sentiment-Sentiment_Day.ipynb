{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14615816-6097-45d6-9ec7-851f1e41d509",
   "metadata": {},
   "source": [
    "# Generate sentiment and emotion by day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32994c70-5ebc-4cfe-b1d0-f282f4441d5a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ddcda27-0ee8-4cf6-bacb-d8544fcdfac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from pathlib import Path\n",
    "# from scraping import create_connection\n",
    "# from tqdm.notebook import tqdm\n",
    "# from math import floor\n",
    "# import lemmatize as l\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "import time\n",
    "from torch import tensor\n",
    "from scipy.special import softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24990fdb-30a6-486b-9bb2-34957f9fcedf",
   "metadata": {},
   "source": [
    "## File Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5da29f7e-3902-4e94-afe0-1cd8fa640c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path.cwd()\n",
    "path_parent = p.parents[1]\n",
    "path_db = str(path_parent / \"database\" / \"netmums-merged.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b2b29a-eb3e-428e-a486-4f6356ff0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_clean_data = path_parent / \"clean_data\" / \"netmums\"\n",
    "path_text_pkl = str(path_clean_data / \"daily_clean_text.pkl\")\n",
    "path_emote_text_pkl = str(path_clean_data / \"daily_emote_clean_text_{}.pkl\")\n",
    "path_emote_processed_pkl = str(path_clean_data / \"daily_emote_processed_{}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6033a469-b28b-4f01-9f3f-5de8aa245d92",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get Daily Text\n",
    "env=forum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e7cfa-e2ee-4684-b9dc-63058011d2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "    SELECT\n",
    "        text.text_clean AS text_clean,\n",
    "        s.name AS subforum_name,\n",
    "        p.user_url AS user_url,\n",
    "        p.date_created AS date_created\n",
    "    FROM text\n",
    "    LEFT JOIN posts AS p\n",
    "    ON text.post_id = p.id\n",
    "    LEFT JOIN threads AS t\n",
    "    ON t.id=p.thread_id\n",
    "    LEFT JOIN subforums AS s\n",
    "    ON s.id=t.subforum_id\n",
    "    LEFT JOIN forums AS f\n",
    "    ON f.id=s.forum_id\n",
    "    WHERE text.text_clean<>\"\"\n",
    "    AND p.user_url<>\"Anonymous\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacaa01c-8bdc-4d60-b951-3a8d6f0f65fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = create_connection(path_db)\n",
    "df = pd.read_sql_query(sql, conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be37df-f7dc-408b-9378-c9f19ce3ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(df, npartitions=200)\n",
    "ddf['date_created'] = dd.to_datetime(ddf['date_created'])\n",
    "ddf['day'] = ddf['date_created'].dt.date\n",
    "ddf = ddf[['user_url', 'day', 'text_clean']].groupby([\"user_url\", \"day\"])[\"text_clean\"].apply(lambda grp: ' '.join(grp), meta=('text', 'object'))\n",
    "df = ddf.compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897a6a36-7f44-41b9-b221-cbbdfde60470",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111fab10-2747-4a96-84e9-51cbf62b350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(path_text_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8b0504-8fbb-4f4a-9e7f-01c6a60c2a74",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bcd9c5-b7bc-44b8-9e48-65bd57b507c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(path_text_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b62a8bd-b0be-4469-898b-e9166f756341",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27be29-54f5-4d94-8c9e-92a470d138a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['text'].apply(lambda x: analyzer.polarity_scores(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4de53f-d400-4263-a69c-435e5f856623",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(path_text_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d996534-941c-4cfd-ad04-251fe01fe653",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chunk Text\n",
    "env=hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a7bd6d-db0b-44e8-9510-a31b76e5f05b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "path_model = '/home/mwh/cardiffnlp/twitter-roberta-base-emotion/'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(path_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79100244-eb48-4c27-817b-6435f4df6c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, max_size):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    # filter long words\n",
    "    text = \" \".join([word for word in text.split(\" \") if len(word) <=25])\n",
    "    # tokenize text\n",
    "    encoded = tokenizer.encode(text)[1:-1]\n",
    "    n_tokens = len(encoded)\n",
    "    if n_tokens >= max_size:\n",
    "        n_chunks = n_tokens // max_size + (n_tokens % max_size > 0) # round up\n",
    "        chunk_size = n_tokens // n_chunks + (n_tokens % n_chunks > 0)\n",
    "        return [[0] + encoded[i:i + chunk_size] + [2] for i in range(0, n_tokens, chunk_size)]\n",
    "    else:\n",
    "        return [[0] + encoded + [2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c979586-a0d4-4193-9a8e-c5b88543fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensor(encoded_list):\n",
    "    output = {\n",
    "        'input_ids': tensor([encoded_list]),\n",
    "        'attention_mask': tensor([[1 for i in encoded_list]])\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06f3d19f-b972-4368-a001-0c72c24c4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion_scores(encoded_tensor):\n",
    "    output = model(**encoded_tensor)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    return list(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94afb7ea-e59e-4659-8e28-609389fe5bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_pickle(path_text_pkl)\n",
    "more_than_one = text_df.groupby(\"user_url\")['day'].count()\n",
    "more_than_one = more_than_one[more_than_one > 1].reset_index(drop=False)\n",
    "text_df = text_df.loc[text_df['user_url'].isin(more_than_one['user_url'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baf09fc0-1b9b-4a5e-bd46-7877d0a3c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining = text_df.shape[0] - 1000000\n",
    "half = remaining // 2\n",
    "start = 1000000\n",
    "ranges = [(1, start, start+half), (2, start+half, start+remaining)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb2abe10-6f8d-468d-a4f7-1d2ce66e3037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting text\n",
      "time: 0.7976126670837402\n",
      "stacking encoded words\n",
      "time: 1.1489242553710937\n",
      "making tensors\n",
      "time: 1.839649740854899\n",
      "get emotion scores\n",
      "time: 800.1931878527006\n",
      "total loop time: 803.9793745160102\n",
      "splitting text\n",
      "time: 0.8340093731880188\n",
      "stacking encoded words\n",
      "time: 1.207130761941274\n",
      "making tensors\n",
      "time: 1.8554017265637717\n",
      "get emotion scores\n",
      "time: 800.7881178657213\n",
      "total loop time: 804.6846597274144\n"
     ]
    }
   ],
   "source": [
    "for i, start, end in ranges:\n",
    "    start_time = time.time()\n",
    "    df = text_df.iloc[start:end].copy()\n",
    "    print(\"splitting text\")\n",
    "    ddf = dd.from_pandas(df, npartitions=200)\n",
    "    ddf['encoded'] = ddf.apply(lambda x: split_text(x['text'], 500), axis=1, meta=list)\n",
    "    df = ddf.compute(scheduler='processes')\n",
    "    df.to_pickle(path_emote_text_pkl.format(i))\n",
    "    split_time = time.time()\n",
    "    print(\"time: {}\".format((split_time - start_time) / 60))\n",
    "    print(\"stacking encoded words\")\n",
    "    df = df.set_index(['user_url', 'day'])['encoded'].apply(pd.Series).stack().reset_index()\n",
    "    df.columns = ['user_url', 'day', 'list_number', 'encoded']\n",
    "    df = df[['user_url','day','encoded']]\n",
    "    df.to_pickle(path_emote_text_pkl.format(i))\n",
    "    stack_time = time.time()\n",
    "    print(\"time: {}\".format((stack_time - split_time) / 60))\n",
    "    print(\"making tensors\")\n",
    "    ddf = dd.from_pandas(df, npartitions=200)\n",
    "    ddf['encoded'] = ddf.apply(lambda x: make_tensor(x['encoded']), axis=1, meta=dict)\n",
    "    df = ddf.compute(scheduler='processes')\n",
    "    df.to_pickle(path_emote_text_pkl.format(i))\n",
    "    tensor_time = time.time()\n",
    "    print(\"time: {}\".format((tensor_time - stack_time) / 60))\n",
    "    print(\"get emotion scores\")\n",
    "    ddf = dd.from_pandas(df, npartitions=10)\n",
    "    ddf['scores'] = ddf.apply(lambda x: get_emotion_scores(x['encoded']), axis=1, meta=list)\n",
    "    df = ddf.compute(scheduler='processes')\n",
    "    df = df[['user_url', 'day', 'scores']]\n",
    "    df[['anger', 'joy', 'optimism', 'sadness']] = pd.DataFrame(df[\"scores\"].to_list(), index=df.index)\n",
    "    df.to_pickle(path_emote_processed_pkl.format(i))\n",
    "    emotion_time = time.time()\n",
    "    print(\"time: {}\".format((emotion_time - tensor_time) / 60))\n",
    "    print(\"total loop time: {}\".format((emotion_time - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b026778-f0fc-4e46-a51d-013f833d7019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_url</th>\n",
       "      <th>day</th>\n",
       "      <th>scores</th>\n",
       "      <th>anger</th>\n",
       "      <th>joy</th>\n",
       "      <th>optimism</th>\n",
       "      <th>sadness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lisa-w-2207</td>\n",
       "      <td>2016-10-12</td>\n",
       "      <td>[0.9061063, 0.0063915206, 0.027199049, 0.06030...</td>\n",
       "      <td>0.906106</td>\n",
       "      <td>0.006392</td>\n",
       "      <td>0.027199</td>\n",
       "      <td>0.060303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lisa-w-2207</td>\n",
       "      <td>2016-10-12</td>\n",
       "      <td>[0.7534732, 0.0097232545, 0.06107918, 0.17572428]</td>\n",
       "      <td>0.753473</td>\n",
       "      <td>0.009723</td>\n",
       "      <td>0.061079</td>\n",
       "      <td>0.175724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lisa1st-b</td>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>[0.0438436, 0.031566557, 0.054847788, 0.8697421]</td>\n",
       "      <td>0.043844</td>\n",
       "      <td>0.031567</td>\n",
       "      <td>0.054848</td>\n",
       "      <td>0.869742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lisa1st-b</td>\n",
       "      <td>2018-11-15</td>\n",
       "      <td>[0.5460365, 0.037408844, 0.010406182, 0.4061484]</td>\n",
       "      <td>0.546036</td>\n",
       "      <td>0.037409</td>\n",
       "      <td>0.010406</td>\n",
       "      <td>0.406148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lisa1st-b</td>\n",
       "      <td>2018-11-15</td>\n",
       "      <td>[0.036367387, 0.014276224, 0.010516538, 0.9388...</td>\n",
       "      <td>0.036367</td>\n",
       "      <td>0.014276</td>\n",
       "      <td>0.010517</td>\n",
       "      <td>0.938840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_url        day                                             scores  \\\n",
       "0  lisa-w-2207 2016-10-12  [0.9061063, 0.0063915206, 0.027199049, 0.06030...   \n",
       "1  lisa-w-2207 2016-10-12  [0.7534732, 0.0097232545, 0.06107918, 0.17572428]   \n",
       "2    lisa1st-b 2018-01-26   [0.0438436, 0.031566557, 0.054847788, 0.8697421]   \n",
       "3    lisa1st-b 2018-11-15   [0.5460365, 0.037408844, 0.010406182, 0.4061484]   \n",
       "4    lisa1st-b 2018-11-15  [0.036367387, 0.014276224, 0.010516538, 0.9388...   \n",
       "\n",
       "      anger       joy  optimism   sadness  \n",
       "0  0.906106  0.006392  0.027199  0.060303  \n",
       "1  0.753473  0.009723  0.061079  0.175724  \n",
       "2  0.043844  0.031567  0.054848  0.869742  \n",
       "3  0.546036  0.037409  0.010406  0.406148  \n",
       "4  0.036367  0.014276  0.010517  0.938840  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
